---
title: "Uncertainty in Non-Linear Regression"
author: "Jeremy Ash"
date: "June 11, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)  # for plotting
library(magrittr) # for more readable code
library(dplyr)    # for the sample_n function
library(RColorBrewer) # for more fancy colour
library(nlstools)
library(minpack.lm)
library(rjags) 
library(reshape)
library(mgcv)
```

## Sample Curve Plots

```{r fig1, fig.height=9, fig.width=9, echo = F, cache = T, fig.cap= 'Example of the non-linear regression fits and interval estimates by four methods. (top) Least squares fit with 95% confidence intervals (red) estimated by linear approximation or parametric bootstrap.  The true function used to generate the data is shown in blue.  (bottom left) Bayesian non linear regression.  Fit is the posterior mean of the regression line (purple dashed) along with a 95% credible interval (red).  (bottom right) Fit estimated by a smoothing spline (purple dashed) with Bayesian 95% credible interval (red).'}

####### Loading in the data

load("DRdata.RData")

par(mfrow = c(2,2))
k <- 2
#doses
x <- sim.ls[[k]]$x
#matrix of responses
Y <- sim.ls[[k]]$y
#true DR curve
mu.true <- sim.ls[[k]]$mu.true
#true DR params
real.p <- sim.ls[[k]]$real.p
#number of DR pairs
m <- sim.ls[[k]]$m
plot(x, y, xlab = "Dose", ylab = "Response", main = "Linear Approximation", ylim = c(0, 1.6))
dat <- data.frame(dose = x, resp = y)

######### non-linear least squares methods
nlm <- nls(resp ~ a + b/(1+exp(-c-d*dose)), data = dat,
           start=list(a = 0.1, b = 1.2, c = -5, d = 10))
nlm.sum <- summary(nlm)

nlm.coef <- coef(nlm)

#### Wald Method
# model fit with linear approximation CI
nlm_fn <- predict(nlm, newdata=dat$dose)
uci <- nlm_fn + 1.96*nlm.sum$sigma
lci <- nlm_fn - 1.96*nlm.sum$sigma
lines(x, mu.true, col="blue")
lines(x, nlm_fn, col=6, lty=2)
lines(x, uci, col = "red")
lines(x, lci, col = "red")
legend("bottomright",c("True DR Curve","Fit","95% CI"),
       lty=c(1,2,1), col = c("blue",6,"red"), cex = 1)

##### Parametric bootstrap

mu.pb <- predict(nlm, newdata=dat$dose)

#Simu.pblate new response data
pb.dat <- replicate(1000,
                 {
                   error <- rnorm(length(mu.pb), mean = 0, sd = nlm.sum$sigma)
                   mu.pb + error
                 })
# Get coefficients for each boostrap replicate
fit.nlm.coef <- function(x) {
  nls(x ~ a + b/(1+exp(-c-d*dose)), data = dat,
      start=list(a = nlm.coef[1], b = nlm.coef[2], c = nlm.coef[3], d = nlm.coef[4])) %>% coef(.)
}
pb.coef <- apply(pb.dat, 2, function(x) fit.nlm.coef(x))

#check results make sense
test.coef <- pb.coef[, 1]
test.mu.pb <- pb.coef[1] + pb.coef[2]/(1+exp(-pb.coef[3]-pb.coef[4]*x))

# Get bootstrap CIs and sd
pb.ci <- apply(pb.coef, 1, function(x) {quantile(x, probs = c(.025, .975))})
pb.ci <- t(pb.ci)
pb.se <- apply(pb.coef, 1, sd)

fit.nlm.pred <- function(x) {
  nls(x ~ a + b/(1+exp(-c-d*dose)), data = dat,
      start=list(a = nlm.coef[1], b = nlm.coef[2], c = nlm.coef[3], d = nlm.coef[4])) %>% predict(., newdata=dat$dose)
}

pb.pred <- apply(pb.dat, 2, function(x) fit.nlm.pred(x))
pb.err <- matrix(NA, ncol = dim(pb.pred)[2], nrow = dim(pb.pred)[1])

for(i in 1:nrow(pb.pred)) {
  pb.err[i, ] <- pb.pred[i, ] - dat$resp[i]
}

pb.pred.ci <- apply(pb.pred, 1, function(x) {quantile(x, probs = c(.025, .975))})
plot(x, y, xlab = "Dose", ylab = "Response", main = "Parametric Bootstrap", ylim = c(0, 1.6))
lines(x, nlm_fn, col=6, lty=2)
lines(x, mu.true, col="blue")
lines(x, pb.pred.ci[1, ], col = "red")
lines(x, pb.pred.ci[2, ], col = "red")
legend("bottomright",c("True DR Curve","Fit","95% CI"),
       lty=c(1,2,1), col = c("blue",6,"red"), cex = 1)

####### Bayesian

model_string <- "model{

   # Likelihood

   for(j in 1:m){
    Y[j]  ~ dnorm(mu[j],inv.var)
    mu[j] <- theta[1] + theta[2]*ilogit((x[j]-theta[3])/theta[4])   
   }

   #Transformations

   theta[1] <- exp(alpha[1]) 
   theta[2] <- exp(alpha[1]) + exp(alpha[2])
   theta[3] <- ilogit(alpha[3])
   theta[4] <- exp(alpha[4])

   #Actualy parameters I want
    
    a <- theta[1] 
    b <- theta[2]
    c <- -theta[3]/theta[4]
    d <- 1/theta[4]

   #Priors

   for(j in 1:4){
    alpha[j] ~ dnorm(0,0.01)
   }

   inv.var ~ dgamma(0.01,0.01)
  }"

model <- jags.model(textConnection(model_string), 
                    data = list(Y=y,m=m,x=x),
                    n.chains=3,quiet=TRUE)

update(model, 10000, progress.bar="none") # burn-in

out <- coda.samples(model, 
         variable.names=c("alpha","theta", "a","b","c","d","mu", "inv.var"), 
         n.iter=50000,thin=10,progress.bar="none")
samps  <- rbind(out[[3]],out[[3]],out[[3]])

alpha <- samps[,2:5]
theta <- samps[,20:23]
mu    <- samps[,10:19]
# pairs(cbind(alpha,theta))

plot(x,y,xlab="Dose",ylab="Response", main = "Bayesian Credible Interval", ylim = c(0, 1.6))

q <- apply(mu,2,quantile,c(0.025,0.50,0.975))
fit <- apply(mu,2,mean)

lines(x, mu.true, col="blue")
lines(x, fit, col=6, lty=2)
lines(x, q[1,], col = "red")
lines(x, q[3,], col = "red")
legend("bottomright",c("True DR Curve","Post Mean","95% CI"),
       lty=c(1,2,1), col = c("blue",6,"red"), cex = 1)

# summary(out)
# effectiveSize(out)
# gelman.plot(out)


####### Smoothing Splines

sp.fit <- gam(resp~s(dose, k=m),data=dat)
sp.fit.sum <- summary(sp.fit)

sp.pred <- predict(sp.fit, dat, se.fit = T)
uci <- sp.pred$fit + (1.96 * sp.pred$se.fit)
lci <- sp.pred$fit - (1.96 * sp.pred$se.fit)

plot(x,y,xlab="Dose",ylab="Response", main = "Smoothing Spline", ylim = c(0, 1.6))

lines(x, mu.true, col="blue")
lines(x, sp.pred$fit, col=6, lty=2)
lines(x, uci, col = "red")
lines(x, lci, col = "red")
legend("bottomright",c("True DR Curve","Fit","95% CI"),
       lty=c(1,2,1), col = c("blue",6,"red"), cex = 1)

```

## Standard error estimate

```{r, include=FALSE, cache = T}
load("DRdata_big.RData")

param.var <- matrix(0,4,3)
fit.var <- vector(length = 3)

for(k in 1:3) {
  x <- sim.ls[[k]]$x
  Y <- sim.ls[[k]]$y
  mu.true <- sim.ls[[k]]$mu.true
  #True model coefficients
  real.p <- sim.ls[[k]]$real.p[1:4]
  m <- sim.ls[[k]]$m
  
  #Set up the matrices to store the results
  pred.param <- matrix(0,n.p,n)
  pred.fit <- matrix(0,m,n)
  
  #actual sample size
  n.s <- 0
  for(i in 1:n){
    dat <- data.frame(dose = x, resp = Y[i, ])
    tryCatch({
        nlm <- nlsLM(resp ~ a + b/(1+exp(-c-d*dose)), data = dat,
                     start=list(a = real.p[1], b = real.p[2],
                                c = real.p[3], d = real.p[4]))
        n.s <- n.s + 1
      }, error = function(e){
    }) 
    nlm.sum <- summary(nlm)
    param.var[, k] <- param.var[, k] + (coef(nlm) - real.p)^2
    fit.var[k] <- fit.var[k] + sum(nlm.sum$residuals^2)
  }
  param.var[, k] <-  param.var[, k]/(n.s)
  fit.var[k] <- fit.var[k]/(n.s*m)
}

true.param.se <- sqrt(param.var)
colnames(true.param.se) <- c("7", "10", "15")
rownames(true.param.se) <- c("a", "b", "c", "d")
true.param.se
write.csv(true.param.se, file = "true_param_se.csv")

true.fit.se <- sqrt(fit.var)
names(true.fit.se) <- c("7", "10", "15")
# why is this increasing as the number of doses increases?
write.csv(true.fit.se, file = "true_fit_se.csv")

true.param.se <- rbind(true.param.se, true.fit.se)
```


# Simulation study


```{r, include=FALSE}
load("DRdata.RData")

n.p <- 5
n.met <- 4
sample.sizes <- m.vec

cover.ls <- list()
width.ls <- list()
mse.ls <- list()
se.ls <- list()
bias.ls <- list()

for(k in 1:3) {
  x <- sim.ls[[k]]$x
  Y <- sim.ls[[k]]$y
  mu.true <- sim.ls[[k]]$mu.true
  real.p <- sim.ls[[k]]$real.p
  m <- sim.ls[[k]]$m
  
  #Set up the matrices to store the results
  cover <- matrix(0,n.p,n.met)
  dimnames(cover)[[2]]<-c("LA","PB", "BIC", "SP")
  dimnames(cover)[[1]]<- c("a", "b", "c", "d", "fit")
  width <- cover
  
  MSE   <- matrix(0,n.p,n.met)
  dimnames(MSE)[[2]]<-c("LA","PB", "BIC", "SP")
  dimnames(MSE)[[1]]<-c("a", "b", "c", "d", "res")
  bias <- MSE
  SEs <- MSE
  
  for(i in 1:n){
  
    dat <- data.frame(dose = x, resp = Y[i, ])
    
    ###########################
    ##### Linear Approximation
    ###########################
    
    #### Wald Method
    # model fit with linear approximation CI
    print(i)
    
    nlm <- nlsLM(resp ~ a + b/(1+exp(-c-d*dose)), data = dat,
             start=list(a = real.p[1], b = real.p[2],
                        c = real.p[3], d = real.p[4]))
    nlm.sum <- summary(nlm)
    nlm.coef <- coef(nlm)
    est.p <- c(nlm.coef, nlm.sum$sigma)
    est.se <- c(nlm.sum$parameters[, 2], res = nlm.sum$sigma)
    
    for(j in 1:n.p) {
    # Estimation
     MSE[j,1]  <- MSE[j,1] + ((est.se[j]-true.param.se[j])^2)/n
     bias[j,1] <- bias[j,1] + (est.se[j]-true.param.se[j])/n
    }
  
    # Intervals
    # Parameters
    
    param.ci <- confint.default(nlm)
    for(j in 1:(n.p-1)) {
      cover[j,1] <- cover[j,1]+(param.ci[j, 1]<=real.p[j] & param.ci[j, 2]>=real.p[j])/n
      width[j,1] <- width[j,1]+(param.ci[j, 2]-param.ci[j, 1])/n
    }
    
    # Function
  
    nlm_fn <- predict(nlm, newdata=dat$dose)
    uci <- nlm_fn + 1.96*nlm.sum$sigma
    lci <- nlm_fn - 1.96*nlm.sum$sigma
    pred.ci <- cbind(lci, uci)
    
    for(j in 1:m) {
      cover[n.p,1] <- cover[n.p,1]+(pred.ci[j, 1]<=mu.true[j] & pred.ci[j, 2]>=mu.true[j])/(n*m)
      width[n.p,1] <- width[n.p,1]+(pred.ci[j, 2]-pred.ci[j, 1])/(n*m)
    }
    
    # In case we need the standard errors
    SEs[, 1] <- SEs[, 1] + c(nlm.sum$parameters[, 2], res = nlm.sum$sigma)/n
    
    ###########################
    ##### Parametric bootstrap
    ###########################
    
    mu.true.pb <- predict(nlm, newdata=dat$dose)
    
    #Simulate new response data
    set.seed(123)
    pb.dat <- replicate(100,
                     {
                       error <- rnorm(length(mu.true.pb), mean = 0, sd = nlm.sum$sigma)
                       mu.true.pb + error
                     })
    # Get coefficients for each boostrap replicate
    fit.nlm.coef <- function(x) {
      tryCatch({
      nlsLM(x ~ a + b/(1+exp(-c-d*dose)), data = dat, 
          start=list(a = nlm.coef[1], b = nlm.coef[2], c = nlm.coef[3], d = nlm.coef[4])) %>% coef(.)
      }, error = function(e){
        rep(NA, 4)
      }) 
    }
    
    pb.coef <- apply(pb.dat, 2, function(x) fit.nlm.coef(x))
    pb.coef <- pb.coef[, apply(pb.coef, 2, function(x) !any(is.na(x)))] 
    
    # Get function for each boostrap replicate
    fit.nlm.pred <- function(x) {
      tryCatch({
      nlsLM(x ~ a + b/(1+exp(-c-d*dose)), data = dat, 
          start=list(a = nlm.coef[1], b = nlm.coef[2], c = nlm.coef[3], d = nlm.coef[4])) %>% predict(., newdata=dat$dose)
      }, error = function(e){
        rep(NA, m)
      })
    }
    pb.pred <- apply(pb.dat, 2, function(x) fit.nlm.pred(x))
    pb.pred <- pb.pred[, apply(pb.pred, 2, function(x) !any(is.na(x)))] 
    
    # Compute MSE and CP
  
    est.p <- apply(pb.coef, 1, mean)
    pb.se <- apply(pb.coef, 1, sd)
    
    pb.err <- matrix(NA, ncol = dim(pb.pred)[2], nrow = dim(pb.pred)[1])
    for(i in 1:nrow(pb.pred)) {
      pb.err[i, ] <- pb.pred[i, ] - dat$resp[i]
    }
   
    res.se <- sd(c(pb.err))
    est.se <- c(pb.se, res = res.se)
    est.p <- c(est.p, res = res.se)
    
    for(j in 1:n.p) {
    # Estimation
     MSE[j,2]  <- MSE[j,2] + ((est.se[j]-true.param.se[j])^2)/n
     bias[j,2] <- bias[j,2] + (est.se[j]-true.param.se[j])/n
    }
  
    # Intervals
    # Parameters
    
    param.ci <- t(apply(pb.coef, 1, function(x) {quantile(x, probs = c(.025, .975))}))
    for(j in 1:(n.p-1)) {
      cover[j,2] <- cover[j,2]+(param.ci[j, 1]<=real.p[j] & param.ci[j, 2]>=real.p[j])/n
      width[j,2] <- width[j,2]+(param.ci[j, 2]-param.ci[j, 1])/n
    }
    
    # Function
    
    pred.ci <- t(apply(pb.pred, 1, function(x) {quantile(x, probs = c(.025, .975))}))
  
    for(j in 1:m) {
      cover[n.p,2] <- cover[n.p,2]+(pred.ci[j, 1]<=mu.true[j] & pred.ci[j, 2]>=mu.true[j])/(n*m)
      width[n.p,2] <- width[n.p,2]+(pred.ci[j, 2]-pred.ci[j, 1])/(n*m)
    }
    
    # In case we need the standard errors
    SEs[, 2] <- SEs[, 2] + est.se/n
    
    ###########################
    ##### Bayesian
    ###########################
    
    model_string <- "model{

   # Likelihood

   for(j in 1:m){
    Y[j]  ~ dnorm(mu[j],inv.var)
    mu[j] <- theta[1] + theta[2]*ilogit((x[j]-theta[3])/theta[4])   
   }

   #Transformations

   theta[1] <- exp(alpha[1]) 
   theta[2] <- exp(alpha[1]) + exp(alpha[2])
   theta[3] <- ilogit(alpha[3])
   theta[4] <- exp(alpha[4])

   #Actualy parameters I want
    
    a <- theta[1] 
    b <- theta[2]
    c <- -theta[3]/theta[4]
    d <- 1/theta[4]

   #Priors

   for(j in 1:4){
    alpha[j] ~ dnorm(0,0.01)
   }

   inv.var ~ dgamma(0.01,0.01)
    sd <- sqrt(1/inv.var)
  }"

  model <- jags.model(textConnection(model_string), 
                      data = list(Y=dat$resp,m=m,x=dat$dose),
                      n.chains=3,quiet=TRUE)
  
  update(model, 10000, progress.bar="none") # burn-in
  
  out <- coda.samples(model, 
           variable.names=c("alpha","theta", "a","b","c","d","mu", "sd"), 
           n.iter=50000,thin=10,progress.bar="none")
  samps  <- rbind(out[[3]],out[[3]],out[[3]])
  
  mu    <- samps[, 9:(9+m-1)]
  params <- samps[, c(1, 6, 7, 8)]
  res.se <- samps[, 9+m]
  
  est.p <- t(apply(params,2,mean))
  se.p <- t(apply(params,2,sd))
  est.p <- c(est.p, mean(res.se))
  est.se <- c(se.p, mean(res.se))
  
  for(j in 1:n.p) {
  # Estimation
   MSE[j,3]  <- MSE[j,3] + ((est.se[j]-true.param.se[j])^2)/n
   bias[j,3] <- bias[j,3] + (est.se[j]-true.param.se[j])/n
  }
  
  # Intervals
  param.ci <- t(apply(params,2,quantile,c(0.025,0.975)))
  for(j in 1:(n.p-1)) {
    cover[j,3] <- cover[j,3]+(param.ci[j, 1]<=real.p[j] & param.ci[j, 2]>=real.p[j])/n
    width[j,3] <- width[j,3]+(param.ci[j, 2]-param.ci[j, 1])/n
  }
  
  pred.ci <- t(apply(mu,2,quantile,c(0.025,0.975)))
  for(j in 1:m) {
    cover[n.p,3] <- cover[n.p,3]+(pred.ci[j, 1]<=mu.true[j] & pred.ci[j, 2]>=mu.true[j])/(n*m)
    width[n.p,3] <- width[n.p,3]+(pred.ci[j, 2]-pred.ci[j, 1])/(n*m)
  }

  SEs[, 3] <- SEs[, 3] + c(se.p, mean(res.se))/n
  
  #############################
  ####### Smoothing Splines
  ################################
  
  sp.fit <- gam(resp~s(dose, k=m),data=dat)
  sp.fit.sum <- summary(sp.fit)
  
  sp.pred <- predict(sp.fit, dat, se.fit = T)
  uci <- sp.pred$fit + (1.96 * sp.pred$se.fit)
  lci <- sp.pred$fit - (1.96 * sp.pred$se.fit)
  # estimate of prediction standard error changes at different
  # points along the curve, average to get single estimate?
  res.se <- mean(sp.pred$se.fit)
  
  MSE[n.p,4]  <- MSE[n.p,4] + ((res.se-true.param.se[n.p, k])^2)/n
  bias[n.p,4] <- bias[n.p,4] + (res.se-true.param.se[n.p, k])/n
  
  # Intervals

  pred.ci <- cbind(lci, uci)
  for(j in 1:m) {
    cover[n.p,4] <- cover[n.p,3]+(pred.ci[j, 1]<=mu.true[j] & pred.ci[j, 2]>=mu.true[j])/(n*m)
    width[n.p,4] <- width[n.p,3]+(pred.ci[j, 2]-pred.ci[j, 1])/(n*m)
  }

  cover.ls[[k]] <- cover
  width.ls[[k]] <- width
  mse.ls[[k]] <- MSE
  se.ls[[k]] <- SEs
  bias.ls[[k]] <- bias
    
  }
}

```

```{r fig2, fig.height=9, fig.width=9, echo = F, fig.cap= 'Coverage probabilities for the interval estimates by four non linear regression methods. 95% confidence/credible intervals were estimated for each of the parameters and the regression line at each of the design points. Coverage probabilities were estimated by monte carlo simulation (100 replicates).  A dashed line shows the nominal 95% confidence level for each of methods. (red) LS linear approximation confidence interval. (blue) LS parametric bootstrap confidence interval.  (purple) Bayesian non linear regression credible interval. (yellow)  Smoothing spline credible interval.'}

col.cds <- c(7, 8, 12, 13) - 1

par(mfrow = c(2, 3))
mets <- c("LA", "PB", "BCI", "SP")
params <- c("a","b","c","d","Function")
for(j in 1:n.p){
  plot(1, type="n", xlab = "Sample Size", ylab = "Coverage Probabilities",
       ylim=c(.7, 1), xlim = c(1-.1, 3+.1), xaxt='n', main = params[j])
  set.seed(132)
  for(i in 1:4){
    cp <- c(cover.ls[[1]][j, i], cover.ls[[2]][j, i], cover.ls[[3]][j, i])
    points(jitter(c(1, 2, 3)), cp, col = col.cds[i], pch = 19)
  }
  abline(a = .95, b = 0,  lty = "dashed")
  legend("bottomright",mets, pch = rep(19, 2),
         col = col.cds, cex = .75)
  axis(1, at = c(1, 2, 3), labels = c(7, 10, 15))
}
```

```{r, eval=FALSE, include=FALSE}

par(mfrow = c(2, 3))
mets <- c("LA", "PB", "BCI", "SP")
params <- c("a","b","c","d","Function")

ylim.ls <- list(c(0, .2), c(0, 1), c(3, 7), c(5, 12), c(0, .3))
for(j in 1:n.p){
  plot(1, type="n", xlab = "Sample Size", ylab = "Average CI Width",
       ylim=ylim.ls[[j]], xlim = c(1-.1, 3+.1), xaxt='n', main = params[j])
  set.seed(133)
  for(i in 1:4){
    cp <- c(width.ls[[1]][j, i], width.ls[[2]][j, i], width.ls[[3]][j, i])
    points(jitter(c(1, 2, 3)), cp, col = col.cds[i], pch = 19)
  }
  legend("bottomright",mets, pch = rep(19, 2),
         col = col.cds, cex = .75)
  axis(1, at = c(1, 2, 3), labels = c(7, 10, 15))
}

par(mfrow = c(2, 3))
mets <- c("LA", "PB", "BCI", "SP")
params <- c("a","b","c","d","Function")

ylim.ls <- list(c(.001, .01), c(.001, .05), c(.001, 3), c(2, 10), c(.00001, .001))
for(j in 1:n.p){
  plot(1, type="n", xlab = "Sample Size", ylab = "Mean Square Error",
       ylim=ylim.ls[[j]], xlim = c(1-.1, 3+.1), xaxt='n', main = params[j])
  set.seed(133)
  for(i in 1:2){
    cp <- c(mse.ls[[1]][j, i], mse.ls[[2]][j, i], mse.ls[[3]][j, i])
    points(jitter(c(1, 2, 3)), cp, col = col.cds[i], pch = 19)
  }
  legend("topright",mets[1:2], pch = rep(19, 2),
         col = col.cds[1:2], cex = .75)
  axis(1, at = c(1, 2, 3), labels = c(7, 10, 15))
}

CPs <- cover[, 1]
plot(rep(10, 5), cover[, 1], pch = 7)
abline(a = .95, b = 0,  lty = "dashed")


mdata <- melt(mydata, id=c("id","time"))



```



