@article{Alorf2017,
author = {Alorf, Abdulkarim A},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alorf - 2017 - K-MEANS, MEAN SHIFT, AND SLIC CLUSTERING ALGORITHMS A COMPARISON OF PERFORMANCE IN COLOR-BASED SKIN SEGMENTATION.pdf:pdf},
title = {{K-MEANS, MEAN SHIFT, AND SLIC CLUSTERING ALGORITHMS: A COMPARISON OF PERFORMANCE IN COLOR-BASED SKIN SEGMENTATION}},
url = {http://d-scholarship.pitt.edu/32379/8/alorfaa{\_}etdPitt2017.pdf},
year = {2017}
}
@article{Amelio,
abstract = {—Normalized mutual information (NMI) is a widely used measure to compare community detection methods. Re-cently, however, the need of adjustment for information theo-retic based measures has been argued because of their tendency in choosing clustering solutions with more communities. In this paper an experimental evaluation is performed to investigate this problem, and an adjustment that scales the values of NMI is proposed. Experiments on synthetic generated networks highlight the unbiased behavior of scaled NMI.},
author = {Amelio, Alessia and Pizzuti, Clara},
doi = {10.1145/2808797.2809344},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Amelio, Pizzuti - Unknown - Is Normalized Mutual Information a Fair Measure for Comparing Community Detection Methods.pdf:pdf},
keywords = {-Normalized Mutual Information,community de-tection,selection bias},
title = {{Is Normalized Mutual Information a Fair Measure for Comparing Community Detection Methods?}},
url = {http://delivery.acm.org/10.1145/2810000/2809344/p1584{\_}amelio.pdf?ip=152.1.45.57{\&}id=2809344{\&}acc=ACTIVE SERVICE{\&}key=6ABC8B4C00F6EE47.4D4702B0C3E38B35.4D4702B0C3E38B35.4D4702B0C3E38B35{\&}{\_}{\_}acm{\_}{\_}=1527186484{\_}93a773c3e21a5bc3ba695741881fcebf}
}
@article{Arora2016,
abstract = {Clustering plays a very vital role in exploring data, creating predictions and to overcome the anomalies in the data. Clusters that contain collateral, identical characteristics in a dataset are grouped using reiterative techniques. As the data in real world is growing day by day so very large datasets with little or no background knowledge can be identified into interesting patterns with clustering. So, in this paper the two most popular clustering algorithms K-Means and K-Medoids are evaluated on dataset transaction10k of KEEL. The input to these algorithms are randomly distributed data points and based on their similarity clusters has been generated. The comparison results show that time taken in cluster head selection and space complexity of overlapping of cluster is much better in K-Medoids than K-Means. Also K-Medoids is better in terms of execution time, non sensitive to outliers and reduces noise as compared to K-Means as it minimizes the sum of dissimilarities of data objects.},
author = {Arora, Preeti and Varshney, Shipra},
doi = {10.1016/j.procs.2016.02.095},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arora, Varshney - 2016 - Analysis of K-Means and K-Medoids Algorithm For Big Data.pdf:pdf},
issn = {1877-0509},
journal = {Procedia Computer Science},
keywords = {Clustering,K-Means,K-Medoids},
pages = {507--512},
title = {{Analysis of K-Means and K-Medoids Algorithm For Big Data}},
url = {www.sciencedirect.com},
volume = {78},
year = {2016}
}
@article{Arthur,
abstract = {The k-means method is a widely used clustering technique that seeks to minimize the average squared distance between points in the same cluster. Although it offers no accuracy guarantees, its simplicity and speed are very appealing in practice. By augmenting k-means with a very simple, ran-domized seeding technique, we obtain an algorithm that is $\Theta$(log k)-competitive with the optimal clustering. Prelim-inary experiments show that our augmentation improves both the speed and the accuracy of k-means, often quite dramatically.},
author = {Arthur, David and Vassilvitskii, Sergei},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arthur, Vassilvitskii - Unknown - k-means The Advantages of Careful Seeding.pdf:pdf},
title = {{k-means++: The Advantages of Careful Seeding}},
url = {http://theory.stanford.edu/{~}sergei/papers/kMeansPP-soda.pdf}
}
@article{Barber2007,
author = {Barber, David},
title = {{Bayesian Reasoning and Machine Learning}},
url = {http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/090310.pdf?utm{\_}source=hackernewsletter{\&}utm{\_}medium=email},
year = {2007}
}
@article{Calinski1974,
abstract = {ISSN: 0090-3272 (Print) (Online) Journal homepage: http://www.tandfonline.com/loi/lsta19 Key Words {\&} Phrases: numerical taxonomy; c l u s t e r analysis; minimum variance (WGSSI criterion for optima2 grouping; approximate grouping procedure; shortest dendrite = minimum spanning tree; variance r a t i o criterion for b e s t number of groups. ABSTRACT A method f o r identifying c l u s t e r s of points i n a m u l t i -dimensional Euclidean space is described and its application t o taxonomy considered. It reconciles, i n a sense, two d i f -f e r e n t approaches t o the investigation of the s p a t i a l rela-tionships between the points, viz., the agglomerative and the divisive methods. A graph, the s h o r t e s t dendrite of Florek e t a l . (l 9 j l a) , is constructed on a nearest neighbour basis ana then divided i n t o c l u s t e r s by applying the c r i t e r i o n of mini-mum within-cluster sum of squares. This procedure ensures an effective reduction of the number of possible s p l i t s . The method may be applied t o a dichotomous division, b u t is per-f e c t l y suitable a l s o f o r a global division i n t o any number of clusters. An informal indicator of the "best number" of clus-t e r s i s suggested. It i s a "variance r a t i o c r i t e r i o n " giving some insight i n t o the structure of the pointa. The method is mean,, electronic or mechanical, including photocopying, microfilming, and recording, or by any information storage and retrieval syshm, wifhou(permission in writing from the publisher.},
author = {Cali{\'{n}}ski, T and Harabasz, J and Caliliski, T},
doi = {10.1080/03610927408827101},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cali{\'{n}}ski, Harabasz, Caliliski - 1974 - Communications in Statistics -Theory and Methods A dendrite method for cluster analysis A DENDRIT.pdf:pdf},
journal = {COMMUNICATIONS IN STATISTICS},
number = {1},
pages = {1--27},
title = {{Communications in Statistics -Theory and Methods A dendrite method for cluster analysis A DENDRITE METHOD FOR CLUSTER ANALYSIS}},
url = {http://www.tandfonline.com/action/journalInformation?journalCode=lsta19 https://doi.org/10.1080/03610927408827101},
volume = {3},
year = {1974}
}
@article{Carreira-Perpinan2015,
abstract = {A natural way to characterize the cluster structure of a dataset is by finding regions containing a high density of data. This can be done in a nonparametric way with a kernel density estimate, whose modes and hence clusters can be found using mean-shift algorithms. We describe the theory and practice behind clustering based on kernel density estimates and mean-shift algorithms. We discuss the blurring and non-blurring versions of mean-shift; theoretical results about mean-shift algorithms and Gaussian mixtures; relations with scale-space theory, spectral clustering and other algorithms; extensions to tracking, to manifold and graph data, and to manifold denoising; K-modes and Laplacian K-modes algorithms; acceleration strategies for large datasets; and applications to image segmentation, manifold denoising and multivalued regression.},
archivePrefix = {arXiv},
arxivId = {arXiv:1503.00687v1},
author = {Carreira-Perpi{\~{n}}{\'{a}}n, Migue{\'{i}} A},
eprint = {arXiv:1503.00687v1},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Carreira-Perpi{\~{n}}{\'{a}}n - 2015 - A review of mean-shift algorithms for clustering.pdf:pdf},
title = {{A review of mean-shift algorithms for clustering *}},
url = {http://eecs.ucmerced.edu},
year = {2015}
}
@article{Chacon2014,
abstract = {Many developments in Mathematics involve the computation of higher order derivatives of Gaussian density functions. The analysis of univariate Gaussian random variables is a well-established field whereas the analysis of their multivariate counterparts consists of a body of results which are more dispersed. These latter results generally fall into two main categories: theoretical expressions which reveal the deep structure of the prob-lem, or computational algorithms which can mask the connections with closely related problems. In this paper, we unify existing results and develop new results in a frame-work which is both conceptually cogent and computationally efficient. We focus on the underlying connections between higher order derivatives of Gaussian density functions, the expected value of products of quadratic forms in Gaussian random variables, and V -statistics of degree two based on Gaussian density functions. These three sets of results are combined into an analysis of non-parametric data smoothers.},
author = {Chac{\'{o}}n, Jos{\'{e}} E and Duong, Tarn},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chac{\'{o}}n, Duong - 2014 - Efficient recursive algorithms for functionals based on higher order derivatives of the multivariate Gaussian den.pdf:pdf},
keywords = {15A24,62E10,62G05,62H05,65F30,Hermite polynomial,derivative,kernel estimator,normal density,quadratic forms,symmetrizer matrix},
title = {{Efficient recursive algorithms for functionals based on higher order derivatives of the multivariate Gaussian density}},
url = {https://arxiv.org/pdf/1310.2559.pdf},
year = {2014}
}
@article{Chacon2013,
abstract = {Important information concerning a multivariate data set , such as clusters and modal regions , is contained in the derivatives of the proba - bility density function . Despitethisimportance,nonparametricestimationofhigherorderderivativesofthedensityfunctionshavereceivedonlyrel-ativelyscantattention.Kernelestimatorsofdensityfunctionsarewidelyusedastheyexhibitexcellenttheoreticalandpracticalproperties,thoughtheirgeneralizationtodensityderivativeshasprogressedmoreslowlyduetothemathematicalintractabilitiesencounteredinthecrucialproblemofbandwidth(orsmoothingparameter)selection.Thispaperpresentsthefirstfullyautomatic,data-basedbandwidthselectorsformultivariatekernelden-sityderivativeestimators.Thisisachievedbysynthesizingrecentadvancesinmatrixanalytictheorywhichallowmathematicallyandcomputationallytractablerepresentationsofhigherorderderivativesofmultivariatevectorvaluedfunctions.Thetheoreticalasymptoticpropertiesaswellasthefi-nitesamplebehaviouroftheproposedselectorsarestudied.Inaddition,weexploreindetailtheapplicationsofthenewdata-drivenmethodsfortwootherstatisticalproblems:clusteringandbumphunting.Theintro-ducedtechniquesarecombinedwiththemeanshiftalgorithmtodevelopnovelautomatic,nonparametricclusteringprocedureswhichareshowntooutperformmixture-modelclusteranalysisandotherrecentnonparametricapproachesinpractice.Furthermore,theadvantageoftheuseofsmooth-ingparametersdesignedfordensityderivativeestimationforfeaturesignif-icanceanalysisforbumphuntingisillustratedwitharealdataexample.},
author = {Chac{\'{o}}n, Jos{\'{e}} E and Duong, Tarn},
doi = {10.1214/13-EJS781},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chac{\'{o}}n, Duong - 2013 - Data-driven density derivative estimation, with applications to nonparametric clustering and bump hunting.pdf:pdf},
issn = {1935-7524},
journal = {Electronic Journal of Statistics},
keywords = {62G05,62H30Adjusted Rand index,cross validation,feature significance,mean integrated squared error,mean shift algorithm,nonparametric kernel method,plug-in choice},
pages = {499--532},
title = {{Data-driven density derivative estimation, with applications to nonparametric clustering and bump hunting}},
url = {http://mvstat.net/tduong/research/publications/chacon-duong-2013-ejs.pdf},
volume = {7},
year = {2013}
}
@article{Chacon,
abstract = {We explore the performance of several automatic bandwidth selectors, originally designed for density gradient estimation, as data-based procedures for non-parametric, modal clustering. The key tool to obtain a clustering from density gradi-ent estimators is the mean shift algorithm, which allows to obtain a partition not only of the data sample, but also of the whole space. The results of our simulation study suggest that most of the methods considered here, like cross validation and plug in bandwidth selectors, are useful for cluster analysis via the mean shift algorithm.},
author = {Chac{\'{o}}n, Jos{\'{e}} E and Monfort, Pablo},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chac{\'{o}}n, Monfort - Unknown - A comparison of bandwidth selectors for mean shift clustering.pdf:pdf},
keywords = {bandwidth selection,mean shift algorithm,modal clustering},
title = {{A comparison of bandwidth selectors for mean shift clustering}},
url = {https://arxiv.org/pdf/1310.7855.pdf}
}
@article{Comaniciu,
abstract = {ÐA general nonparametric technique is proposed for the analysis of a complex multimodal feature space and to delineate arbitrarily shaped clusters in it. The basic computational module of the technique is an old pattern recognition procedure, the mean shift. We prove for discrete data the convergence of a recursive mean shift procedure to the nearest stationary point of the underlying density function and, thus, its utility in detecting the modes of the density. The relation of the mean shift procedure to the Nadaraya-Watson estimator from kernel regression and the robust M-estimators of location is also established. Algorithms for two low-level vision tasks, discontinuity preserving smoothing and image segmentation, are described as applications. In these algorithms, the only user set parameter is the resolution of the analysis and either gray level or color images are accepted as input. Extensive experimental results illustrate their excellent performance.},
author = {Comaniciu, Dorin and Meer, Peter},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Comaniciu, Meer - Unknown - Mean Shift A Robust Approach Toward Feature Space Analysis.pdf:pdf},
keywords = {Index TermsÐMean shift,clustering,feature space,image segmentation,image smoothing,low-level vision},
title = {{Mean Shift: A Robust Approach Toward Feature Space Analysis}},
url = {https://courses.csail.mit.edu/6.869/handouts/PAMIMeanshift.pdf}
}
@article{Cornell2015,
author = {Cornell, Devin and Sastry, Sushruth},
file = {:C$\backslash$:/Users/jrash/Downloads/ee6540{\_}final{\_}project-1.pdf:pdf},
number = {May},
title = {{Performance Comparison of K-Means and Expectation Maximization with Gaussian Mixture Models for Clustering}},
url = {https://dcornellresearch.org/2015/10/30/performance-comparison-of-k-means-and-expectation-maximization-with-gaussian-mixture-models-for-clustering/},
year = {2015}
}
@article{Dempster1977,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. SUMMARY A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
author = {Dempster, A P and Laird, N M and Rubin, D B},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dempster, Laird, Rubin - 1977 - Maximum Likelihood from Incomplete Data via the EM Algorithm.pdf:pdf},
journal = {Source Journal of the Royal Statistical Society. Series B (Methodological)},
keywords = {EM ALGORITHM,INCOMPLETE DATA,MAXIMUM LIKELIHOOD,POSTERIOR MODE},
number = {1},
pages = {1--38},
title = {{Maximum Likelihood from Incomplete Data via the EM Algorithm}},
url = {http://www.jstor.org/stable/2984875 http://about.jstor.org/terms},
volume = {39},
year = {1977}
}
@article{Fowlkes1983,
author = {Fowlkes, E B and Mallows, C L},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fowlkes, Mallows - 1983 - A Method for Comparing Two Hierarchical Clusterings.pdf:pdf},
journal = {Source Journal of the American Statistical Association},
number = {383},
pages = {553--569},
publisher = {American Statistical Association},
title = {{A Method for Comparing Two Hierarchical Clusterings}},
url = {http://www.jstor.org/stable/2288117 http://dv1litvip.jstor.org/ http://www.jstor.org/action/showPublisher?publisherCode=astata.},
volume = {78},
year = {1983}
}
@article{Fukunaga1975,
abstract = {Nonparametric density gradient estimation using a generalized kernel approach is investigated. Conditions on the kernel functions are derived to guarantee asymptotic unbiasedness, consistency, and uniform consistency of the estimates. The results are generalized to obtain a simple mcan-shift estimate that can be extended in a{\textless}tex{\textgreater}k{\textless}/tex{\textgreater}-nearest-neighbor approach. Applications of gradient estimation to pattern recognition are presented using clustering and intrinsic dimensionality problems, with the ultimate goal of providing further understanding of these problems in terms of density gradients.},
author = {Fukunaga, Keinosuke and Hostetler, Larry D.},
doi = {10.1109/TIT.1975.1055330},
file = {:C$\backslash$:/Users/jrash/Downloads/01055330.pdf:pdf},
isbn = {0018-9448},
issn = {15579654},
journal = {IEEE Transactions on Information Theory},
number = {1},
pages = {32--40},
title = {{The Estimation of the Gradient of a Density Function, with Applications in Pattern Recognition}},
volume = {21},
year = {1975}
}
@article{Gallant2011,
abstract = {The design, measurement, and analysis of a range of artificial materials$\backslash$nfor use at terahertz frequencies are described. The chosen structures$\backslash$nconsist of arrays of cylindrical gold-plated pillars with period$\backslash$ncomparable to the wavelength of incident radiation. An ultraviolet (UV)$\backslash$nmicromachining approach to the fabrication of these high aspect-ratio$\backslash$npillars is described using the negative epoxy-based resin SU8. Lattice$\backslash$nfence structures are also realized using the same method. Terahertz$\backslash$n(THz) frequency time domain spectroscopy is performed on these$\backslash$nstructures in the range 200 GHz to 3.0 THz and the relative transmission$\backslash$nof the structures is determined. The pass and stop bands are observed$\backslash$nwith peak transmission of up to 97{\%}. Finite difference time domain$\backslash$nsimulations and complex photonic band structure calculations are shown$\backslash$nto provide good descriptions of the electromagnetic properties of the$\backslash$nstructures and are used to interpret the observed transmission spectra.$\backslash$n(c) American Institute of Physics.},
archivePrefix = {arXiv},
arxivId = {arXiv:0811.2183v2},
author = {Gallant, a. J. and Kaliteevski, M. a. and Brand, S. and Wood, D. and Petty, M. and Abram, R. a. and Chamberlain, J. M.},
doi = {10.1063/1.2756072},
eprint = {arXiv:0811.2183v2},
file = {:C$\backslash$:/Users/jrash/Downloads/288-1102-1-PB.pdf:pdf},
isbn = {9783642121425},
issn = {00218979},
journal = {Journal of Applied Physics},
keywords = {coverage,goodness-of-fit,local principal curves,mean shift clustering},
number = {October},
pages = {0--103},
pmid = {23347591},
title = {{Bandwidth Selection for Mean-shift based Unsupervised Learning Techniques: a Unified Approach via Self-coverage}},
url = {http://dx.doi.org/10.1016/j.worlddev.2005.07.015},
volume = {102},
year = {2011}
}
@article{Hamerly,
author = {Hamerly, Greg and Elkan, Charles and {   " Y Yp D Q}, F},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hamerly, Elkan,   Y Yp D Q - Unknown - Alternatives to the k-means algorithm that find better clusterings.pdf:pdf},
title = {{Alternatives to the k-means algorithm that find better clusterings}},
url = {http://people.csail.mit.edu/tieu/notebook/kmeans/cikm02.pdf}
}
@article{Hennig2004,
abstract = {ML-estimation based on mixtures of Normal distributions is a widely used tool for cluster analysis. However, a single outlier can make the parameter estimation of at least one of the mixture components break down. Among others, the estimation of mixtures of t-distributions by McLachlan and Peel [Finite Mixture Models (2000) Wiley, New York] and the addition of a further mixture component accounting for " noise " by Fraley and Raftery [The Computer J. 41 (1998) 578–588] were suggested as more robust alternatives. In this paper, the definition of an adequate robustness measure for cluster analysis is discussed and bounds for the breakdown points of the mentioned methods are given. It turns out that the two alternatives, while adding stability in the presence of outliers of moderate size, do not possess a substantially better breakdown behavior than estimation based on Normal mixtures. If the number of clusters s is treated as fixed, r additional points suffice for all three methods to let the parameters of r clusters explode. Only in the case of r = s is this not possible for t-mixtures. The ability to estimate the number of mixture components, for example, by use of the Bayesian information criterion of Schwarz [Ann. Statist. 6 (1978) 461–464], and to isolate gross outliers as clusters of one point, is crucial for an improved breakdown behavior of all three techniques. Furthermore, a mixture of Normals with an improper uniform distribution is proposed to achieve more robustness in the case of a fixed number of components.},
author = {Hennig, Christian},
doi = {10.1214/009053604000000571},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hennig - 2004 - BREAKDOWN POINTS FOR MAXIMUM LIKELIHOOD ESTIMATORS OF LOCATION–SCALE MIXTURES.pdf:pdf},
journal = {The Annals of Statistics},
number = {4},
pages = {1313--1340},
title = {{BREAKDOWN POINTS FOR MAXIMUM LIKELIHOOD ESTIMATORS OF LOCATION–SCALE MIXTURES}},
url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.aos/1091626171},
volume = {32},
year = {2004}
}
@article{Hubert1985,
abstract = {The problem of comparing two different partitions of a finite set of objects reappears continually in the clustering literature. We begin by review-ing a well-known measure of partition correspondence often attributed to Rand (1971), discuss the issue of correcting this index for chance, and note that a recent normalization strategy developed by Morey and Agresti (1984) and adopted by others (e.g., Milligan and Cooper 1985) is based on an incorrect assumption. Then, the general problem of comparing partitions is approached indirectly by assessing the congruence of two proximity matrices using a simple cross-product measure. They are generated from cOrrespond-ing partitions using various scoring rules. Special cases derivable include traditionally familiar statistics and/or ones tailored to weight certain object pairs differentially. Finally, we propose a measure based on the comparison of object triples having the advantage of a probabilistic interpretation in addi-tion to being corrected for chance (i.e., assuming a constant value under a reasonable null hypothesis) and bounded between -4-I.},
author = {Hubert, Lawrence and Arabic, Phipps},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hubert, Arabic - 1985 - Comparing Partitions(2).pdf:pdf},
journal = {Journal of Classification},
keywords = {Consensus indices,Measures of agreement,Measures of association},
pages = {193--218},
title = {{Comparing Partitions}},
volume = {2},
year = {1985}
}
@article{Hyrien2016,
abstract = {Mean-shift is an iterative procedure often used as a nonparametric clustering algorithm that defines clusters based on the modal regions of a density function. The algorithm is conceptually appealing and makes assumptions neither about the shape of the clusters nor about their number. However, with a complexity of O(n2) per iteration, it does not scale well to large data sets. We propose a novel algorithm which performs density-based clustering much quicker than mean-shift, yet delivering virtually identical results. This algorithm combines subsampling and a stochastic approximation procedure to achieve a potential complexity of O(n) at each step. Its convergence is established. Its performances are evaluated using simulations and applications to image segmentation, where the algorithm was tens or hundreds of times faster than mean-shift, yet causing negligible amounts of clustering errors. The algorithm can be combined with existing approaches to further accelerate clustering.},
author = {Hyrien, Ollivier and Baran, Andrea},
doi = {10.1080/10618600.2015.1051625},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hyrien, Baran - 2016 - Fast Nonparametric Density-Based Clustering of Large Data Sets Using a Stochastic Approximation Mean-Shift Algori.pdf:pdf},
issn = {1061-8600},
journal = {Journal of computational and graphical statistics : a joint publication of American Statistical Association, Institute of Mathematical Statistics, Interface Foundation of North America},
keywords = {Image segmentation,Large data sets,Robbins-Monro procedure},
number = {3},
pages = {899--916},
pmid = {28479847},
publisher = {NIH Public Access},
title = {{Fast Nonparametric Density-Based Clustering of Large Data Sets Using a Stochastic Approximation Mean-Shift Algorithm.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/28479847 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5417725},
volume = {25},
year = {2016}
}
@article{Liu2013,
abstract = {a r t i c l e i n f o a b s t r a c t The Mean-Shift (MS) algorithm and its variants have wide applications in pattern recognition and computer vision tasks such as clustering, segmentation, and tracking. In this paper, we study the dynamics of the algorithm with Gaussian kernels, based on a Generalized MS (GMS) model that includes the standard MS as a special case. First, we prove that the GMS has solutions in the convex hull of the given data points. By the principle of contraction mapping, a sufficient condition, dependent on a parameter introduced into Gaussian kernels, is provided to guarantee the uniqueness of the solution. It is shown that the solution is also globally stable and exponentially convergent under the condition. When the condition does not hold, the GMS algorithm can possibly have multiple equilibriums, which can be used for clustering as each equilibrium has its own attractive basin. Based on this, the condition can be used to estimate an appropriate parameter which ensures the GMS algorithm to have its equilibriums suitable for clustering. Examples are given to illustrate the correctness of the condition. It is also shown that the use of the multiple-equilibrium property for clustering, on the data sets such as IRIS, leads to a lower error rate than the standard MS approach, and the K-Means and Fuzzy C-Means algorithms.},
author = {Liu, Yiguang and Li, Stan Z and Wu, Wei and Huang, Ronggang},
doi = {10.1016/j.ipl.2012.10.002},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liu et al. - 2013 - Dynamics of a mean-shift-like algorithm and its applications on clustering.pdf:pdf},
journal = {Information Processing Letters},
keywords = {Clustering,Design of algorithms,Exponential convergence,Mean-shift algorithm,Stability},
pages = {8--16},
title = {{Dynamics of a mean-shift-like algorithm and its applications on clustering}},
url = {www.elsevier.com/locate/ipl},
volume = {113},
year = {2013}
}
@article{Lloyd1982,
abstract = {129 Abstract-It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantiza-tion noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quantization schemes for 26 quanta, b = 1,2, t ,7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
author = {Lloyd, Stuart P},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lloyd - 1982 - Least Squares Quantization in PCM.pdf:pdf},
journal = {IEEE TRANSACTIONS ON INFORMATION THEORY},
number = {2},
title = {{Least Squares Quantization in PCM}},
url = {http://www.cs.cmu.edu/{~}bhiksha/courses/mlsp.fall2010/class14/lloyd.pdf},
year = {1982}
}
@book{McLachlan2008,
abstract = {2nd ed. This new edition remains the only single source to offer a complete and unified treatment of the theory, methodology, and applications of the EM algorithm. The highly applied area of statistics here outlined involves applications in regression, medical im. Examples of the EM algorithm -- Basic theory of the EM algorithm -- Standard errors and speeding up convergence -- Extensions of the EM algorithm -- Monte Carlo versions of the EM algorithm -- Some generalizations of the EM algorithm -- Further applications of the EM algorithm.},
author = {McLachlan, Geoffrey J. and Krishnan, T. (Thriyambakam)},
isbn = {0470191600},
pages = {359},
publisher = {Wiley-Interscience},
title = {{The EM algorithm and extensions}},
url = {https://books.google.com/books?hl=en{\&}lr={\&}id=NBawzaWoWa8C{\&}oi=fnd{\&}pg=PR3{\&}dq=The+EM+Algorithm+and+Extensions{\&}ots=to89PRXCvM{\&}sig=sWJA3JAMpkgQvEbj3phHgOaMMss{\#}v=onepage{\&}q=The EM Algorithm and Extensions{\&}f=false},
year = {2008}
}
@article{Meila2007,
abstract = {This paper proposes an information theoretic criterion for comparing two partitions, or clusterings, of the same data set. The criterion, called variation of information (VI), measures the amount of information lost and gained in changing from clustering C to clustering C′. The basic properties of VI are presented and discussed. We focus on two kinds of properties: (1) those that help one build intuition about the new criterion (in particular, it is shown the VI is a true metric on the space of clusterings), and (2) those that pertain to the comparability of VI values over different experimental conditions. As the latter properties have rarely been discussed explicitly before, other existing comparison criteria are also examined in their light. Finally we present the VI from an axiomatic point of view, showing that it is the only “sensible” criterion for comparing partitions that is both aligned to the lattice and convexely additive. As a consequence, we prove an impossibility result for comparing partitions: there is no criterion for comparing partitions that simultaneously satisfies the above two desirable properties and is bounded.},
author = {Meilă, Marina},
doi = {10.1016/J.JMVA.2006.11.013},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Meilă - 2007 - Comparing clusterings—an information based distance.pdf:pdf},
issn = {0047-259X},
journal = {Journal of Multivariate Analysis},
month = {may},
number = {5},
pages = {873--895},
publisher = {Academic Press},
title = {{Comparing clusterings—an information based distance}},
url = {https://www.sciencedirect.com/science/article/pii/S0047259X06002016},
volume = {98},
year = {2007}
}
@article{Milligan1986,
author = {Milligan, G W and and Cooper, M C},
doi = {10.1207/s15327906mbr2104},
file = {:C$\backslash$:/Users/jrash/Downloads/milligan1986.pdf:pdf},
journal = {Multivariate Behavioral Research},
number = {March 2015},
pages = {441--458},
title = {{A study of the comparability of externalcriteria for hierarchical cluster analysis}},
volume = {21},
year = {1986}
}
@article{Milligan1985,
author = {Milligan, Glenn W. and Cooper, Martha C.},
doi = {10.1007/BF02294245},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Milligan, Cooper - 1985 - An examination of procedures for determining the number of clusters in a data set.pdf:pdf},
issn = {0033-3123},
journal = {Psychometrika},
month = {jun},
number = {2},
pages = {159--179},
publisher = {Springer-Verlag},
title = {{An examination of procedures for determining the number of clusters in a data set}},
url = {http://link.springer.com/10.1007/BF02294245},
volume = {50},
year = {1985}
}
@article{Morey1984,
abstract = {Investigators examining empirically derived classifications are often concerned with the replicability of an obtained classification. However, most available statistics which allow replication comparison suffer from various limitations. This paper proposes an adjustment to one of these statistics, the Rand statistic, which will allow comparison across different levels for number of clusters found within a classification. This adjustment permits the user to compare several different classifications with respect to classification agreement, while correcting for the contribution of chance to any observed agreement.},
annote = {doi: 10.1177/0013164484441003},
author = {Morey, Leslie C and Agresti, Alan},
doi = {10.1177/0013164484441003},
issn = {0013-1644},
journal = {Educational and Psychological Measurement},
month = {mar},
number = {1},
pages = {33--37},
publisher = {SAGE Publications Inc},
title = {{The Measurement of Classification Agreement: An Adjustment to the Rand Statistic for Chance Agreement}},
url = {https://doi.org/10.1177/0013164484441003},
volume = {44},
year = {1984}
}
@article{Overview1975,
author = {Overview, History and Types, Details and Clustering, Applications and Smoothing, Tracking and Weaknesses, Strengths and See, Availability and Ghassabeh, Aliyari},
file = {:C$\backslash$:/Users/jrash/Downloads/Mean{\_}shift.pdf:pdf},
title = {{Mean shift}},
year = {1975}
}
@book{P.Murphy1991,
abstract = {Some of the most remarkable issues related to interharmonics observed from a probabilistic perspective are presented. Attention is firstly devoted to interharmonic frequency and amplitude variability. Starting from the basic mathematical and computational aspects of probabilistic harmonic models, the difficulties to include interharmonics are discussed with particular attention to the problem of the frequency resolution and of the computational burden. Then, simulation and measurement aspects are discussed, also showing some numerical and experimental results.},
archivePrefix = {arXiv},
arxivId = {0-387-31073-8},
author = {{P. Murphy}, Kevin},
booktitle = {Machine Learning: A Probabilistic Perspective},
doi = {10.1007/SpringerReference_35834},
eprint = {0-387-31073-8},
file = {:C$\backslash$:/Users/jrash/Downloads/(Adaptive Computation and Machine Learning) Kevin P. Murphy-Machine Learning{\_} A Probabilistic Perspective-The MIT Press (2012).pdf:pdf},
isbn = {9780262018029},
issn = {0262018020},
pages = {73--78,216--244},
pmid = {20236947},
title = {{Machine Learning: A Probabilistic Perspective}},
url = {http://link.springer.com/chapter/10.1007/978-94-011-3532-0{\_}2},
year = {1991}
}
@article{Parameters2016,
author = {Parameters, Intuitive and Clusters, Stable},
file = {:C$\backslash$:/Users/jrash/Downloads/hdbscan-readthedocs-io-en-latest-comparing{\_}clustering{\_}algorithms-html.pdf:pdf},
pages = {1--12},
title = {{Comparing Python Clustering Algorithms}},
year = {2016}
}
@article{Park1990,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. This article compares several promising data-driven methods for selecting the bandwidth of a kernel density estimator. The methods compared are least squares cross-validation, biased cross-validation, and a plug-in rule. The comparison is done by asymptotic rate of convergence to the optimum and a simulation study. It is seen that the plug-in bandwidth is usually most efficient when the underlying density is sufficiently smooth, but is less robust when there is not enough smoothness present. We believe the plug-in rule is the best of those currently available, but there is still room for improvement.},
author = {Park, Byeong U and Marron, J S},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Park, Marron - 1990 - Comparison of Data-Driven Bandwidth Selectors.pdf:pdf},
journal = {Source Journal of the American Statistical Association},
keywords = {Cross-validation,Data-driven bandwidth selection,Density estimation,Kernel estimators,Plug-in method},
number = {409},
pages = {66--72},
title = {{Comparison of Data-Driven Bandwidth Selectors}},
url = {http://www.jstor.org/stable/2289526 http://about.jstor.org/terms},
volume = {85},
year = {1990}
}
@article{Rand1971,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Many intuitively appealing methods have been suggested for clustering data) however, interpretation of their results has been hindered by the lack of objective criteria. This article proposes several criteria which isolate specific aspects of the performance of a method, such as its retrieval of inherent structure, its sensitivity to resampling and the stability of its results in the light of new data. These criteria depend on a measure of similarity between two different clusterings of the same set of data; the measure essentially considers how each pair of data points is assigned in each clustering.},
author = {Rand, William M},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rand - 1971 - Objective Criteria for the Evaluation of Clustering Methods(3).pdf:pdf},
journal = {Source Journal of the American Statistical Association Journal of the American Statistical Association},
number = {66},
pages = {846--850},
title = {{Objective Criteria for the Evaluation of Clustering Methods}},
url = {http://www.jstor.org/stable/2284239 http://about.jstor.org/terms},
volume = {66},
year = {1971}
}
@article{Rousseeuw1987,
abstract = {A new graphical display is proposed for partitioning techniques. Each cluster is represented by a so-called silhouette, which is based on the comparison of its tightness and separation. This silhouette shows which objects lie well within their cluster, and which ones are merely somewhere in between clusters. The entire clustering is displayed by combining the silhouettes into a single plot, allowing an appreciation of the relative quality of the clusters and an overview of the data configuration. The average silhouette width provides an evaluation of clustering validity, and might be used to select an 'appropriate' number of clusters. {\textcopyright} 1987.},
archivePrefix = {arXiv},
arxivId = {z0024},
author = {Rousseeuw, Peter J.},
doi = {10.1016/0377-0427(87)90125-7},
eprint = {z0024},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rousseeuw - 1987 - Silhouettes A graphical aid to the interpretation and validation of cluster analysis(2).pdf:pdf},
isbn = {03770427},
issn = {03770427},
journal = {Journal of Computational and Applied Mathematics},
keywords = {Graphical display,classification,cluster analysis,clustering validity},
number = {C},
pages = {53--65},
pmid = {19382406},
title = {{Silhouettes: A graphical aid to the interpretation and validation of cluster analysis}},
volume = {20},
year = {1987}
}
@article{Scrucca,
abstract = {Finite mixture models are being used increasingly to model a wide variety of random phenomena for clustering, classification and density estimation. mclust is a powerful and popular package which allows modelling of data as a Gaussian finite mixture with different covariance structures and different numbers of mixture components, for a variety of purposes of analysis. Recently, version 5 of the package has been made available on CRAN. This updated version adds new covariance structures, dimension reduction capabilities for visualisation, model selection criteria, initialisation strategies for the EM algorithm, and bootstrap-based inference, making it a full-featured R package for data analysis via finite mixture modelling.},
author = {Scrucca, Luca and Fop, Michael and Murphy, T Brendan and Raftery, Adrian E},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Scrucca et al. - Unknown - mclust 5 Clustering, Classification and Density Estimation Using Gaussian Finite Mixture Models.pdf:pdf},
title = {{mclust 5: Clustering, Classification and Density Estimation Using Gaussian Finite Mixture Models}},
url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5096736/pdf/nihms793803.pdf}
}
@article{Singh2003,
abstract = {This paper explores the novel application of two vector quantization algorithms, namely Linde, Buzo, Gray (1980) and K-means algorithm for efficient speaker verification. Automatic speaker verification (ASV) is a memory and compute intensive process, giving rise to area and latency concerns in the way of its implementation for real-time efficient embedded systems. The training schemes for computing the speaker models, such as the expectation maximization are highly iterative and contribute significantly to the overall complexity in the implementation of the system. We demonstrate the use of the LBG and the K-means algorithm to realize compute efficient training method. Models trained with the LBG algorithm achieves as much as 99.88{\%} of EM accuracy, whilst K-means achieves as much as 99.91{\%} of EM accuracy. Moreover, the EM computational complexity is almost twice that of LBG or K-means. Thus, using LBG and K-means algorithms for training Gaussian mixture speaker models for text-independent speaker verification, we show that, that they deliver comparable performance as the EM algorithm at significantly reduced computational complexity. Thus making them an ideal choice for low-cost applications.},
author = {Singh, G and Panda, A and Bhattacharyya, S and Srikanthan, T},
doi = {10.1109/ICASSP.2003.1202295},
file = {:C$\backslash$:/Users/jrash/Downloads/01202295.pdf:pdf},
isbn = {0780376633},
issn = {1520-6149},
journal = {Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03). 2003 IEEE International Conference on},
keywords = {Gaussian processes;computational complexity;embedd},
pages = {II--65--8 vol.2},
title = {{Vector quantization techniques for GMM based speaker verification}},
volume = {2},
year = {2003}
}
@article{Wang2007,
abstract = {A novel non-parametric clustering method based on non-parametric local shrinking is proposed. Each data point is transformed in such a way that it moves a specific distance toward a cluster center. The direction and the associated size of each movement are determined by the median of its K-nearest neighbors. This process is repeated until a pre-defined convergence criterion is satisfied. The optimal value of the number of neighbors is determined by optimizing some commonly used index functions that measure the strengths of clusters generated by the algorithm. The number of clusters and the final partition are determined automatically without any input parameter except the stopping rule for convergence. Experiments on simulated and real data sets suggest that the proposed algorithm achieves relatively high accuracies when compared with classical clustering algorithms.},
author = {Wang, Xiaogang and Qiu, Weiliang and Zamar, Ruben H},
doi = {10.1016/j.csda.2006.12.016},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wang, Qiu, Zamar - 2007 - CLUES A non-parametric clustering method based on local shrinking.pdf:pdf},
journal = {Computational Statistics {\&} Data Analysis},
keywords = {Automatic clustering,K-nearest neighbors,Local shrinking,Number of clusters},
pages = {286--298},
title = {{CLUES: A non-parametric clustering method based on local shrinking}},
url = {www.elsevier.com/locate/csda},
volume = {52},
year = {2007}
}
@article{Xu1995,
abstract = {We build up the mathematical connection between the $\backslash$Expectation-Maximization" (EM) algorithm and gradient-based approaches for maximum likelihood learning of fnite Gaussian mixtures. We show that the EM step in parameter space is obtained from the gradient via a projection matrix P, a n d w e provide an explicit expression for the matrix. We then analyze the convergence of EM in terms of special properties of P and provide new results analyzing the eeect that P has on the likelihood surface. Based on these mathematical results, we present a comparative discussion of the advantages and disadvantages of EM and other algorithms for the learning of Gaussian mixture models. contract N00000-00-A-0000. The authors were also supported by the HK RGC Earmarked Grant CUHK250/94E, by a g r a n t from the McDonnell-Pew Foundation, by a g r a n t f r o m A TR Human Information Processing Research Laboratories, by a g r a n t from Siemens Corporation, and by g r a n t N00014-90-1-0777 from the OOce of Naval Research. Michael I. Jordan is an NSF Presidential Young Investigator.},
author = {Xu, Lei and Jordan, Michael I},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Jordan - 1995 - On Convergence Properties of the EM Algorithm for Gaussian Mixtures.pdf:pdf},
title = {{On Convergence Properties of the EM Algorithm for Gaussian Mixtures}},
url = {https://dspace.mit.edu/bitstream/handle/1721.1/7195/aim-1520.pdf?sequence=2},
year = {1995}
}
@misc{Zhang,
abstract = {We propose a new class of center-based iterative clustering algorithms, K-Harmonic Means (KHM p), which is essentially insensitive to the initialization of the centers, demonstrated through many experiments. The insensitivity to initialization is attributed to a dynamic weighting function, which increases the importance of the data points that are far from any centers in the next iteration. The dependency of the K-Means' and EM's performance on the initialization of the centers has been a major problem. Many have tried to generate good initializations to solve the sensitivity problem. KHM p addresses the intrinsic problem by replacing the minimum distance from a data point to the centers, used in K-Means, by the Harmonic Averages of the distances from the data point to all centers. KHM p significantly improves the quality of clustering results comparing with both K-Means and EM. The KHM p algorithms have been implemented in both sequential and parallel languages and tested on hundreds of randomly generated datasets with different data distribution and clustering characteristics.},
author = {Zhang, Bin},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang - Unknown - Generalized K-Harmonic Means --Dynamic Weighting of Data in Unsupervised Learning.pdf:pdf},
keywords = {Data Mining,Dynamic Modeling,EM,K-Harmonic Means,K-Means,– Clustering},
title = {{Generalized K-Harmonic Means --Dynamic Weighting of Data in Unsupervised Learning}},
url = {https://pdfs.semanticscholar.org/b051/fa5e441c1bfa72013e7a559d62a03e15b14e.pdf},
urldate = {2018-05-23}
}
@article{Zhang1999,
abstract = {Data clustering is one of the common techniques used in data mining. A popular performance f unction for measuring goodness of data clustering is the total within-cluster variance, or the total mean-square quantization error (MSE). The K-Means (KM) algorithm is a popular algorithm which attempts to find a K -clustering which minimizes MSE. The K -Means algorithm is a center-based clustering algorithm. The dependency of the K-Means performance on the initialization of the centers is a major problem; a similar issue exists for an alternative algorithm, Expectation Maximization (EM), although to a lesser extent. In this paper, we propose a new clustering method called the K -Harmonic Means algorithm (KHM). KHM is a center-based clustering algorithm which uses the Harmonic Averages of the distances from each data point to the centers as components to its performance function. It is demonstrated that K-Harmonic Means is essentially insensitive to the initialization of the centers. In certain cases, K-Harmonic Means significantly improves the quality of clustering results comparing with both K-Means and EM, wh ich are the two most popular clustering algorithms used in data exploration and data compression. A unified view of the three performance functions, K-Means', K-Harmonic Means' and EM's, are given for comparison. Experimental results of KHM comparing with KM on high dimensional data and visualization of the animation of the convergence of all three algorithms using 2-dimensional data are given. Abstract Data clustering is one of the common techniques used in data mining. A popular performance function for measuring goodness of data clustering is the total within-cluster variance, or the total mean-square quantization error (MSE). The K-Means (KM) algorithm is a popular algorithm which attempts to find a K-clustering which minimizes MSE. The K-Means algorithm is a center-based clustering algorithm. The dependency of the K-Means performance on the initialization of the centers is a major problem; a similar issue exists for an alternative algorithm, Expectation Maximization(EM), although to a lesser extent. In this paper, we propose a new clustering method called the K-Harmonic Means algorithm (KHM). KHM is a center-based clustering algorithm which uses the Harmonic Averages of the distances from each data point to the centers as components to its performance function. It is demonstrated that K-Harmonic Means is essentially insensitive to the initialization of the centers. In certain cases, K-Harmonic Means significantly improves the quality of clustering results comparing with both K-Means and EM, which are the two most popular clustering algorithms used in data exploration and data compression. A unified view of the three performance functions, K-Means', K-Harmonic Means' and EM's, are given for comparison. Experimental results of KHM comparing with KM on high dimensional data and visualization of the animation of the convergence of all three algorithms using 2-dimensional data are given.},
author = {Zhang, Bin and Hsu, Meichun and Dayal, Umeshwar},
file = {:C$\backslash$:/Users/jrash/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang, Hsu, Dayal - 1999 - K-Harmonic Means -A Data Clustering Algorithm.pdf:pdf},
keywords = {Data Mining,K-Harmonic Means,K-Means,– Clustering},
title = {{K-Harmonic Means -A Data Clustering Algorithm}},
url = {http://www.hpl.hp.com/techreports/1999/HPL-1999-124.pdf},
year = {1999}
}
@misc{Hettich,
title = {{Hettich, S. and Bay, S. D. (1999). The UCI KDD Archive [http://kdd.ics.uci.edu]. Irvine, CA: University of California, Department of Information and Computer Science.}},
url = {}
}
