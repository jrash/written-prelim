---
title: "Accumulation Curve Probabilistic Framework"
author: "Jeremy Ash"
date: "May 31, 2018"
output: pdf_document
bibliography: my_collection.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
library(mclust)
library(MASS)
library(mclust)
library(pracma)
library(clues)
library(NMI)
library(mcclust)
library(fpc)
library(MeanShift)
library(knitr)
library(formattable)
library(magrittr)
library(kableExtra)
library(mixtools)  #for ellipse
library(LPCM)
old.par <- par()
```

## Part I

As stated in the prompt, S is the score from a ranking algorithm, where larger values of S are indicative
of the $+$ class and smaller values are indicative of the $-$ class.  

$S|+$ has cumulative distribution function (cdf), $F_+(t)$  and probability distribution function (pdf), $f_+(t)$. $S|-$ has cdf, $F_-(t)$,  and pdf, $f_-(t)$. 

$X = I(active)$, where $I(\cdot)$ is the indicator function.  That is, $X=1$ when an item belongs to the $+$ class and $X = 0$ when an item belongs to the $-$ class. 

S is a random variable drawn from the mariginal distribution of $F_S(t)$.  Since the class that S belongs to is unknown, we treat X as a random variable, where $P(X = 1) = \pi$. 

### Binormal

Under the binormal model, the conditional distributions of $S|+$ and $S|-$ are given by:

$$
\begin{aligned}
S|+ &\sim N(\mu_+, \sigma^2_+)\\
S|- &\sim N(\mu_-, \sigma^2_-)
\end{aligned}
$$
Where $Normal(\mu, \sigma^2)$ is the Normal distribution with mean $\mu$ and  variance $\sigma^2$.

First, I derive the marginal pdf of S:

$$
\begin{aligned}
F_S(t) = P(S < t) &= P((S < t \cap +) \cup (S < t \cap -)) \quad\quad \textrm{+ and - partition the sample space}\\
&= P((S < t \cap +) + P(S < t \cap -)) \quad\quad \textrm{+ and - are mutually exclusive}\\
&= P(+)P(S < t | +) + P(-)P(S < t | -)) \quad\quad \textrm{law of conditional probability}\\
&= \pi F_+(t) + (1-\pi)F_-(t)
\end{aligned}
$$
Taking the derivative wrt t provides the following mixture as the mariginal pdf of S:
$$
f_S(t) = \pi f_+(t) + (1-\pi)f_-(t)
$$

For the binormal model, this is a mixture of Gaussians:

$$
\begin{aligned}
S &\sim \pi N(\mu_+, \sigma^2_+) + (1-\pi)N(\mu_-, \sigma^2_-)\\
\end{aligned}
$$

Next, I derive the conditional distribution of $X|S=t$.  To do so, I note that $X|S = 1$ with probability $\theta = P(+|S=t)$ and $X|S = 0$ with probability $1-\theta$, so $X|S \sim Bern(\theta)$.  $P(+|S=t)$ is known since $S$ is known.  It can be computed as follows:

$$
\begin{aligned}
P(+|S = t) &= \frac{P(+)P(S=t|+)}{P(S=t)} \quad \textrm{(Bayes rule)} \\
&= \frac{\pi f_+(S=t)}{f(S=t)}\\
&= \frac{\pi}{\pi + (1-\pi)\frac{f_-(S=t)}{f_+(S=t)}}
\end{aligned}
$$
Another expression for $\theta$ is precision, which is plotted by the population precision recall curve as a function of $F_+(\cdot)$, or true positive rate, $tpr$.

In **Figure 1**, I illustrate the distribution of $S$ and $X|S$ under a binormal model for $S$ where $u_+ > u_-$.  This means the ranking algorithm has generated scores such that large scores are indicative of the $+$ class and small scores are indicative of the $-$ class.  Two mixing probabilities are considered, .2 and .5. $\pi = .2$ corresponds to the case when the negative class has four times more observations than the positive class.  This is a scenario that occurs frequently in real world data sets such as case control studies or drug discovery settings where the chemicals with a desired activity are rare.

```{r fig1, echo = F, fig.cap= 'Densities of S and X|S for the binormal model. (top left) The pdf of $S|+$ (black), $S|-$ (blue), and $S$ (red) when $u_+ = .6$, $u_- = .4$, $\\sigma_+ = \\sigma_- = .1$, and $\\pi = .2$. (top right) pdfs when $S|+$ and $S|-$ have the same parameters and $\\pi = .5$. (bottom left) The pdf of $X|S = t$, when $t = .8$ and $\\pi = .5$ (bottom right) Precision, or $P(+|S = t)$ for a range of values of $t \\in [0,1]$ when $\\pi = .5$'}
par(mfrow =c(2,2))
t <- seq(0, 1, by = .0001)
pi <- .2
p.plus <- dnorm(t, mean = 0.5, sd = .1)
p.neg <- dnorm(t, mean = 0.45, sd = .1)
plot(t, p.plus, type = "l", xlab = "S", ylab = "Density")
lines(t, p.neg, type = "l", col = "blue")
p.s <- pi*p.plus + (1-pi)*p.neg
lines(t, p.s, type = "l", col = "red")
# legend("topright", legend=c("S|+", "S|-", "S"),
#        col=c("black", "blue", "red"), lty = 1)

t <- seq(0, 1, by = .0001)
pi <- .5
p.plus <- dnorm(t, mean = 0.5, sd = .1)
p.neg <- dnorm(t, mean = 0.45, sd = .1)
plot(t, p.plus, type = "l", xlab = "S", ylab = "Density")
lines(t, p.neg, type = "l", col = "blue")
p.s <- pi*p.plus + (1-pi)*p.neg
lines(t, p.s, type = "l", col = "red")
# legend("topright", legend=c("S|+", "S|-", "S"),
#        col=c("black", "blue", "red"), lty = 1)

t <- seq(0, 1, by = .0001)
pi <- .5
p.plus <- dnorm(t, mean = 0.5, sd = .1)
p.neg <- dnorm(t, mean = 0.45, sd = .1)
p.s <- pi*p.plus + (1-pi)*p.neg
p.active.1 <- c(pi*(p.plus/p.s)[8001], 1-(pi*p.plus/p.s)[8001])
names(p.active.1) <- c("Positive", "Negative")
barplot(p.active.1, col = "grey")

p.active <- pi*p.plus/p.s  
plot(t, p.active, type = "l", xlab = "t", ylab = "Precision")
```

### Bibeta

Under the bibeta model, conditional distributions of $S|+$ and $S|-$ are given by:

$$
\begin{aligned}
S|+ &\sim Beta(\alpha_+, \beta_+)\\
S|- &\sim Beta(\alpha_-, \beta_-)
\end{aligned}
$$
Where $Beta(\alpha, \beta)$ is the Beta distribution with shape parameters $\alpha$ and $\beta$.

The same arguements above can be used to prove that, under the bibeta model, the marginal pdf of $S$ is mixture of Beta distributions:

$$
S \sim \pi Beta(\alpha_+, \beta_+) + (1-\pi)Beta(\alpha_-, \beta_-)
$$

```{r fig2, echo = F, fig.cap= 'Densities of S and X|S for the bibeta model. (top left) The pdf of $S|+$ (black), $S|-$ (blue), and $S$ (red) when $\\alpha_+ = 5$, $\\beta_+ = 1$, $\\alpha_- = 1$, $\\beta_- = 5$ and $\\pi = .2$. (top right) pdfs when $S|+$ and $S|-$ have the same parameters and $\\pi = .5$. (bottom left) The pdf of $X|S = t$, when $t = .8$ and $\\pi = .5$ (bottom right) Precision, or $P(+|S = t)$ for a range of values of $t \\in [0,1]$ when $\\pi = .5$'}
par(mfrow =c(2,2))
t <- seq(0, 1, by = .0001)
pi <- .2
p.plus <- dbeta(t, shape1 = 5, shape2 = 1)
p.neg <- dbeta(t, shape1 = 1, shape2 = 5)
plot(t, p.plus, type = "l", xlab = "S", ylab = "Density")
lines(t, p.neg, type = "l", col = "blue")
p.s <- pi*p.plus + (1-pi)*p.neg
lines(t, p.s, type = "l", col = "red")

t <- seq(0, 1, by = .0001)
pi <- .5
p.plus <- dbeta(t, shape1 = 5, shape2 = 1)
p.neg <- dbeta(t, shape1 = 1, shape2 = 5)
plot(t, p.plus, type = "l", xlab = "S", ylab = "Density")
lines(t, p.neg, type = "l", col = "blue")
p.s <- pi*p.plus + (1-pi)*p.neg
lines(t, p.s, type = "l", col = "red")

t <- seq(0, 1, by = .0001)
pi <- .5
p.plus <- dbeta(t, shape1 = 5, shape2 = 1)
p.neg <- dbeta(t, shape1 = 1, shape2 = 5)
p.s <- pi*p.plus + (1-pi)*p.neg
p.active.1 <- c(pi*(p.plus/p.s)[8001], 1-(pi*p.plus/p.s)[8001])
names(p.active.1) <- c("Positive", "Negative")
barplot(p.active.1, col = "grey")


p.active <- pi*p.plus/p.s  
plot(t, p.active, type = "l", xlab = "t", ylab = "Precision")
```

Again, $X|S \sim Bern(\theta)$, where $\theta = \frac{\pi}{\pi + (1-\pi)\frac{f_-(S=t)}{f_+(S=t)}}$ can be determined by computing the corresponding Beta densities at $S = t$.

In **Figure 2**, I illustrate the distribution of S and X|S under a bibeta model where $\alpha_+ = 5$, $\beta_+ = 1$, $\alpha_- = 1$, $\beta_- = 5$.  $S|+$ has a heavy left skew, while $S|-$ has a heavy right skew. Again, the ranking algorithm has generated scores such that large scores are indicative of the $+$ class and small scores are indicative of the $-$ class.  This scoring algorithm is performing better than the binormal model, because the probability that an item is in the positive class given that the score is high (e.g. above .5) is higher under this model.  **Figures 1 and 2** show an instance of this when $S = .8$. The probability that an item is positive given $S = .8$ is much larger under the bibeta model. 

## Part II

The accumulation curve plots the cumulative fraction of actives, $y_{acc}$, identified as a function of the relative ranks of items, $x_{acc}$, according to some scoring algorithm.  The prompt states that the cumulative fraction of actives identified should be used since we are considering an infinite population.  I argue that a *percentile rank* should be used for $x_{acc}$, where the $(100i)$th percentile ranked item, $i \in (0,1]$, has $100i$ percent of scores greater than or equal to it.  As on the y-axis of the accumulation curve, the percentile is simply a rescaling of the rank of the compound. At percentile rank, $x_{acc}$, all compounds in the precentile are "tested" and the cumulative fraction of actives determined, $y_{acc}$.  Another way of determining this percentile would be to chose a threshold t such that the fraction of items with $S > t$ is $x_{acc}$. 

Borrowing notation from Hughes-Oliver [@Hughes-oliver2000].  We consider an independent random sample of scores $\{S_{i}; i = 1, ..., n\}$ from the marginal distribution of S.  Let $\{S_{i}^+; i = 1, ..., n^+\}$ be the scores that were independently sampled from the $+$ class mixture component, $F_+(\cdot)$, and $\{S_{i}^-; i = 1, ..., n^-\}$ be the scores that were independently sampled from the $-$ class mixture component, $F_-(\cdot)$.  We define, $\hat{F}(\cdot)$, $\hat{F}_+(\cdot)$, $\hat{F}_-(\cdot)$ to be the empirical cumlative distribution functions (cdf) for all, $+$, and $-$ scores: 

$$
\begin{aligned}
\hat{F}(t) &= \frac{1}{n}\sum_{i=1}^{n}I(S_i \leq t)\\
\hat{F}_+(t) &= \frac{1}{n^+}\sum_{i=1}^{n^+}I(S_i^+ \leq t)\\
\hat{F_-}(t) &= \frac{1}{n_-}\sum_{i=1}^{n^-}I(S_i^- \leq t)\\
\end{aligned}
$$

We then chose a threshold $t$ such that $x_{acc} = 1-\hat{F}(t)$ is the precentile selected for testing. The fraction of the active compounds that were correctly predicted to be active  (i.e. $S_{i}^+ > t$) is $y_{acc} = 1-\hat{F}_+(t)$.  This is called the empirical true positive fraction.

Since we are considering a population of infinite size, we consider the convergence of empirical cdf $1-\hat{F^n}(t)$ as $n \rightarrow \infty$ and $1-\hat{F^{n^+}_+}(t)$ as $n^+ \rightarrow \infty$. We assume that the mixing porportion is $0 < \pi <1$ so that $n^+ \rightarrow \infty$ as $n \rightarrow \infty$.

By the Glivenko-Cantelli theorem [@Shuster1972], as the number of independently and identically distributed samples goes to infinity, $\hat{F_n}(t)$ converges to $F(t)$ almost surely.  That is, for every $\epsilon > 0$ and all $t \in [0,1]$.

$$
P(\underset{n \rightarrow \infty}{lim}|\hat{F_n}(t) - F(t)| \leq \epsilon) = 1
$$

Therefore we conclude that 
$$
\begin{aligned}
x_{acc} &= 1-\hat{F^n}(t) \overset{a.s.}{\rightarrow} 1-F(t) = P(S > t)\\
y_{acc} &= 1-\hat{F^{n^+}_+}(t) \overset{a.s.}{\rightarrow} 1-F_+(t) = P(S > t | +)
\end{aligned}
$$
For a population of infinite size, the population accumulation curve plots the pairs $\{P(S > t), P(S > t | +)\}$.  If
$\pi = 0$ or $\pi = 1$, then the accumulation curve actually plots $\{P(S > t), 0\}$ and $\{P(S > t), P(S > t)\}$, respectively.

### Binormal

To plot the population accumulation curve under the binormal model for a given t, we compute:

$$
\begin{aligned}
&P(S>t) = \pi (1- \Phi(\frac{t - \mu_+}{\sigma_+^2})) + (1- \pi)(1- \Phi(\frac{t - \mu_-}{\sigma_-^2})) \\
&P(S>t|+) = 1- \Phi(\frac{t - \mu_+}{\sigma_+^2})
\end{aligned}
$$

Where $\Phi(t)$ is the standard normal cdf.

### Bibeta

To plot the population accumulation curve under the binormal model for a given t, we compute:

$$
\begin{aligned}
&P(S>t) = \pi (1- I_t(\alpha_+, \beta_+)) + (1- \pi)(1- I_t(\alpha_-, \beta_-))\\
&P(S>t|+) = 1- I_t(\alpha_+, \beta_+)
\end{aligned}
$$

Where $I_t(\alpha, \beta)$ is the regularized incomplete beta function: 

$$
I_t(\alpha, \beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}\int_0^t x^{\alpha-1}(1-x)^{\beta-1}
$$

### Functional form of the accumulation curve

We can write $t = F^{-1}_S(x_{acc})$, were $F^{-1}(\cdot)$ is the generalized inverse of $F(\cdot)$.  This allows us to write $y_{acc}$ as a function of $x_{acc}$:

$$
y_{ACC}(x_{ACC}) =  1 -F_+(F_S^{-1}(1-x_{ACC})) 
$$
I will refer to this as the population accumulation curve $ACC(x)$.  This curve can be estimated using the empirical cdfs specified above:

$$
\widehat{ACC}(x) = 1 - \hat{F}_+(\hat{F}_S^{-1}(1-x))
$$

Hsieh and Turnbull [@Hsieh1996] have derived uniform consistency and strong approximation properties of the empirical ROC curve, $1- \hat{F}_+(\hat{F}_-^{-1}(1-x))$. Since their proofs are for general distributions $F_-$ and $F_+$ that satisfy conditions that the corresponding distributions, $F_+$ and $F_S$, in the ACC curve also satisfy.  I assume that uniform consistency
of the functional form of the ACC curve holds, that is, for some $\epsilon > 0$.

$$
\underset{x \in [\epsilon,1-\epsilon]}{sup} |\widehat{ACC}(x) - ACC(x)| \rightarrow 0 \quad \textrm{a.s. as  } n \rightarrow 0
$$

And that:

$$
\sqrt{n}(\widehat{ACC}(x) - ACC(x)) \overset{d}{\rightarrow} N(0, \sigma^2(x))\
$$

I attempted to derive the form of $\sigma^2(x)$ the asymptotic variance of empirial ACC curve, but I am not confident in this.
Instead, I will state how I expect the variance to change as a number of variables change.  I expect that the variance will have two components like the empirical ROC asymptotic variance (see [@Pepe2003]):

$$
var(\widehat{ROC}(t)) =  (\frac{f_+(F_+^{-1}(1-x))}{f_-(F_+^{-1}(1-x))})^2  \frac{x_{ROC}(1-x_{ROC})}{n^+}+\frac{y_{ROC}(1-y_{ROC})}{n^-}
$$

The first term is the variability due to selection of the threshold, which in the ACC curve should depend on $n$ and not only 
$n^+$. I expect that the slope term in the ACC curve will resemble $\frac{f_S(F_S^{-1}(1-x))}{f_+(F_S^{-1}(1-x))}$.  The second term is the variance due to uncertainity in the false positive rate ($fpr$) on the y-axis, which in the ACC curve is the true positive rate ($tpr$), so in the ACC curve this should depend on $n+$.  From what I have seen in simulations, I expect the sampling variance of $\widehat{ACC}(x)$ to increase when the skewness increases, so this term should be included in the expression. 

I expect the sampling variance of $\widehat{ACC}(x)$ to increase when the skewness increases, to increase as $y_{ACC}$ or $x_{ACC}$ approaches .5, and to decrease as $n$ gets larger.  I also expect variance to be larger for the beta distribution than the normal distribution because the slope term in the variance tends to be larger for the beta distribution.  These are the conditions I will checking for during my simulations.


<!-- Since we are considering an infinite population  -->
<!-- Similarily the  -->

<!-- Ignoring class membership and combining all scores as {Si; i = 1, . . . ,n}, -->
<!-- n -->
<!-- also define ? -->
<!-- F(t) = 1 -->
<!-- ? -->
<!-- n -->
<!-- I(Si ??? t). -->

<!-- At the population level, the expected fraction of the data that with score above threshold, $t$, is  -->
<!-- $$ -->
<!-- E[I(S > t)] = Pr(S > t)*1 + Pr(S \leq t)*0 = Pr(S > t) -->
<!-- $$ -->
<!-- which is is plotted on the x-axis of the population accumulation curve.  -->

<!-- The empirical accumulation curve plots the relative rank of compounds on the x-axis.  This compound and all compounds ranked above are prioritized for testing.  Consider the ith ranked compound with score $S_i$.  One could choose to prioritize these compounds by testing all compounds with $S > t$, where $t \in (S_i, S_{i+1}]$. That is, we choose a t that is below the score of the ith ranked compound and above or equal to the score of the (i+1)th ranked compound. The number of compounds tested can be thought of as function of $t$.  We can then scale the x-axis to plot the fraction of the data tested, or the empirical proportion of $S > t$.  The x-axis of the empirical accumulation curve can be thought of as an estimator of the expected fraction of $S > t$, or $Pr(S > t)$.  -->

<!-- As stated in the prompt, the y-axis of the accumulation curve can be scaled such that it plots the cumulative fraction of actives as a function of the x-axis, or the fraction of the data set prioritized for testing.  Since the x-axis is a function of the threshold, t, so is y-axis.  Therefore, the y-axis plots the proportion of actives that were correctly identified as actives, or the actives with $S > t$. This is the empirical true positive rate, $tpr$. This is an estimator of the population false positive rate, $P(S>t|+)$. -->

<!-- To recapitulate, -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- x_{ACC} = \hat{P}(S>t)\\ -->
<!-- y_{ACC} = \hat{P}(S>t|+) -->
<!-- \end{aligned} -->
<!-- $$ -->

## Part III

$\{S_i; i = 1, ..., n\}$ were independently sampled from the marginal distributions of S derived in **Part 1** under both the binormal and bibeta models with $N = 1000, 5000, 10000$ and $\pi = {1/101, 1/11, 1/2}$.   We define skew as $\frac{1-\pi}{\pi}$.  Then, the values of $\pi$ used in this simulation correspond to skew = 100, 10, 1. This was when the sample size of the negative class is 100, 10, and 1 fold greater than the positive class. These sample sizes and skew values were selected because the accumulation curve is frequently used to assess the performance of a classifier in situations where there is large sample sizes and extreme class imbalance.  For example, in drug discovery, data sets will typically contain tens or hundreds of thousands of compounds, while only small fraction will be active.  Extreme skewness was incorporated into the simulation study to assess how reasonable the empirical estimator of the ACC curve is in these situations.

The scores were sampled from the mixture distribution of S as follows: one of the mixture components ($+$, $-$) were selected at random where $P(+) = \pi$ and $P(-) = 1- \pi$. Then, a score was sampled from the distribution corresponding to the sampled class, $f_+(\cdot)$ or $f_-(\cdot)$.  

<!-- A class label was only sampled as a means to sample S from its marginal distribution. Once S was sampled, its class label is still unknown.  To sample a class label for $S_i$, $X$ was sampled from the marginal distribution of $X|S_i$ also derived for the binormal model in **Part 1**.  -->

The sampled scores were then ranked from highest to lowest and at each rank, or number of tests, the fraction of the total actives identified was plotted. Let T be the set of thresholds that that identify the percentile of each ranked compound.  That is, for each $i = 1, ..., n$, we choose a $t$ that is below the score of the $i$th ranked compound and above or equal to the score of the $(i+1)$th ranked compound. This provides an empirical estimator of the population accumulation curve:

$$
\begin{aligned}
\widehat{ACC}(t) &= \{\textrm{proportion of }S \textrm{ values > t, proportion of } S^+ \textrm{ values > t}; t \in T\}\\
&= \{1-\hat{F}_S(t),\ 1-\hat{F}_+(t); t \in T \}
\end{aligned}
$$

### Binormal

We assume the binormal model with, $\mu_+ = .6$, $\mu_- = .4$, $\sigma_+ = \sigma_- = 1$.

As we have shown in Part II, the empirical ACC curve converges almost surely to the population ACC curve
$\forall t \in [0,1]$ as $n \rightarrow \infty$.  Therefore, I expect that by plotting the empirical ACC curve along with the 
population binormal accumulation curve derived in **Part 2**, I will be able to visually see pointwise convergence as n becomes larger. **Figure 3** shows the estimation of the population ACC curve for single simulation replicate when $n$ is moderate but $n^+$ is small ($n = 1000$, $\pi = 1/101$).  We can consider specifying a point on the ACC curve in a number of different ways.  One way would be to specify a threshold and then compute 
$\{P(S > t), P(S > t|+) \}$. At a specific threshold value, $t$, the empirical estimate of $P(S > t)$ is very good and has clearly converged pointwise.  This is because $n$ is sufficiently large.   However, the empirical estimate of $P(S > t|+)$ is poor because the estimate of this relies on $n^+ \approx 10$.  The empirical cdf for $S|+$ is a step function that has clearly not converged to the smooth population cdf.  This results in considerable sampling variability in $\widehat{ACC}(x)$ and a poor approximation of the population curve.  Interestingly, the very early on in the $\widehat{ACC}(x)$ is a good estimate of the population accumulation curve, which is usually the region of greatest interest to researchers, as there will only be resources to test the very top percentile.  Since both $x_{acc}$ and $y_{acc}$ are close to zero in this region, the sample variance of $\widehat{ACC}(x)$ is small.

**Figure 4** shows that either decreasing the skew to 1 or increasing the sample size by 10 fold will dramatically improve the estimation of the population curve. However, when $N = 10,000 and \pi = \frac{1}{101}$, $\widehat{ACC}(x)$ undershoots the population curve by a good margin.  An even larger sample size would be necessary to accurately capture the population curve when there is this much class imbalance.

```{r, fig.height=10, fig.width=9, fig.cap= 'Estimation of the population ACC curve under the binormal model with small sample size (N = 1,000) and large skew ($\\pi = 1/101$). $u_+ = .6$, $u_- = .4$, $\\sigma_+ = \\sigma_- = .1$.  (top left) The pdf of $S$ (red), $S|+$ (black) and $S|-$ (blue). (top right) Empirical cdf used to estimate P(S > t) (red) and the true population curve (black) as t ranges from 0 to 1.(bottom left) Empirical cdf used to estimate P(S > t | +) (red) and the true population curve (black).  (bottom right) The empirical estimate of the ACC curve (red) and the true population curve (black).'}

#####################
# small pi
#####################

# when pi is small there is very large variation in the sample accumulation curve
# this is because the confidence interval for P(S > t | +) should be large
# at any value t due to the small sample size
# The sample accumulation curve always underestimates the true population curve
# This is because early on in the curve the confidence intervals are large, and you
# are more likely to miss below the curve
par(mfrow = c(2,2))

t <- seq(0, 1, by = .0001)
pi <- 1/101
p.plus <- dnorm(t, mean = 0.6, sd = .1)
p.neg <- dnorm(t, mean = 0.4, sd = .1)
plot(t, p.plus, type = "l", xlab = "S", ylab = "Density")
lines(t, p.neg, type = "l", col = "blue")
p.s <- pi*p.plus + (1-pi)*p.neg
lines(t, p.s, type = "l", col = "red")

set.seed(123)
N <- 1000
pi <- 1/101

x <- vector(length = N)
s <- vector(length = N)

# setting up mixture parameters
components <- sample(1:2, prob=c(pi, 1-pi), size=N, replace=TRUE)
mus <- c(.6, .4)
sd.s <- .1

# simulate 2
s <- rnorm(n = N, mean = mus[components], sd = sd.s)

# simulate x
p.plus <- dnorm(s, mean = mus[1], sd = sd.s)
p.neg <- dnorm(s, mean = mus[2], sd = sd.s)
p.s <- pi*p.plus + (1-pi)*p.neg
p.active <- pi*p.plus/p.s  
x <- rbinom(n=N, size = 1, p.active)

t <- seq(1, 0, -.01)
p <- ecdf(s)

emp.p.s <- 1 - p(t)

######## P(S > t)

# Plot of the empirical P(S > t)
plot(t, emp.p.s, type = "l", col = "red", lty = "dashed", 
     xlab = "t", ylab = "P(S > t)", lwd=1.5)

# Plot the actual P(S > t)
p.plus <- pnorm(t, mean = mus[1], sd = sd.s, lower.tail = F)
p.neg <- pnorm(t, mean = mus[2], sd = sd.s, lower.tail = F)
true.p.s <- pi*p.plus + (1-pi)*p.neg
lines(t, true.p.s, type = "l")

####### P(S > t | +)

# Plot of the empirical P(S > t | +)
s.plus <- s[components==1]
p <- ecdf(s.plus)
emp.p.s.plus <- 1 - p(t)
plot(t, emp.p.s.plus, type = "l", col = "red", lty = "dashed", 
     xlab = "t", ylab = "P(S > t | +)", lwd=1.5)

# Plot of the true P(S > t | +)
lines(t, p.plus, type = "l", lwd=1.5)

plot(true.p.s, p.plus, type = "l", xlim = c(0,.5),
      xlab = "P(S > t)", ylab = "P(S > t | +)", lwd=1.5)
lines(emp.p.s, emp.p.s.plus, type = "l", col = "red", lty = "dashed", lwd=1.5)
```


```{r, fig.height=8, fig.width=8, fig.cap= 'Estimation of the population ACC curve under the same binormal model as Figure 1 with different combinations of sample size and $\\pi$. (left) Empirical ACC curve estimate when N = 1,000 and $\\pi = 1/2$. (right) Empirical ACC curve estimate when N = 10,000 and $\\pi = 1/101$.'}

#####################
# other curves
#####################

###### N = 1000, pi = .2
par(mfrow = c(1,2))
set.seed(123)
N <- 1000
pi <- .5

x <- vector(length = N)
s <- vector(length = N)

# setting up mixture parameters
components <- sample(1:2, prob=c(pi, 1-pi), size=N, replace=TRUE)
# mus <- c(.7, .3)
# sd.s <- .1

# simulate 2
s <- rnorm(n = N, mean = mus[components], sd = sd.s)

# simulate x
p.plus <- dnorm(s, mean = mus[1], sd = sd.s)
p.neg <- dnorm(s, mean = mus[2], sd = sd.s)
p.s <- pi*p.plus + (1-pi)*p.neg
p.active <- pi*p.plus/p.s  
x <- rbinom(n=N, size = 1, p.active)

t <- seq(1, 0, -.01)
p <- ecdf(s)
emp.p.s <- 1 - p(t)

######## P(S > t)

# Plot of the empirical P(S > t)
# Plot the actual P(S > t)
p.plus <- pnorm(t, mean = mus[1], sd = sd.s, lower.tail = F)
p.neg <- pnorm(t, mean = mus[2], sd = sd.s, lower.tail = F)
true.p.s <- pi*p.plus + (1-pi)*p.neg

####### P(S > t | +)

# Plot of the empirical P(S > t | +)
s.plus <- s[components==1]
p <- ecdf(s.plus)
emp.p.s.plus <- 1 - p(t)

# Plot of the ACC curve
plot(true.p.s, p.plus, type = "l", xlim = c(0,.7),
      xlab = "P(S > t)", ylab = "P(S > t | +)", lwd=1.5)
lines(emp.p.s, emp.p.s.plus, type = "l", col = "red", lty = "dashed", lwd=1.5)


###################################################


####### N = 10000, pi = 1/101

set.seed(123)
N <- 10000
pi <- 1/101

x <- vector(length = N)
s <- vector(length = N)

# setting up mixture parameters
components <- sample(1:2, prob=c(pi, 1-pi), size=N, replace=TRUE)
# mus <- c(.7, .3)
# sd.s <- .1

# simulate 2
s <- rnorm(n = N, mean = mus[components], sd = sd.s)

# simulate x
p.plus <- dnorm(s, mean = mus[1], sd = sd.s)
p.neg <- dnorm(s, mean = mus[2], sd = sd.s)
p.s <- pi*p.plus + (1-pi)*p.neg
p.active <- pi*p.plus/p.s  
x <- rbinom(n=N, size = 1, p.active)

t <- seq(1, 0, -.01)
p <- ecdf(s)
emp.p.s <- 1 - p(t)

######## P(S > t)

# Plot of the empirical P(S > t)
# Plot the actual P(S > t)
p.plus <- pnorm(t, mean = mus[1], sd = sd.s, lower.tail = F)
p.neg <- pnorm(t, mean = mus[2], sd = sd.s, lower.tail = F)
true.p.s <- pi*p.plus + (1-pi)*p.neg

####### P(S > t | +)

# Plot of the empirical P(S > t | +)
s.plus <- s[components==1]
p <- ecdf(s.plus)
emp.p.s.plus <- 1 - p(t)

# Plot of the ACC curve
plot(true.p.s, p.plus, type = "l", xlim = c(0,.3),
      xlab = "P(S > t)", ylab = "P(S > t | +)", lwd=1.5)
lines(emp.p.s, emp.p.s.plus, type = "l", col = "red", lty = "dashed", lwd=1.5)

```

## Bibeta

We assume the bibeta model as illustrated previously, with $\alpha_+ = 5$, $\beta_+ = 1$, $\alpha_- = 1$, $\beta_- = 5$.  Again, the mixing proportions used were $\pi = {1/101, 1/11, 1/2}$.

The empirical ACC curve was visually inspected for pointwise convergence as N became larger. **Figure 5** shows the results when $N=1000$ $\pi = 1/11$.  The population accumulation curve is much more optimal in this scenario.  $P(S > t|+)$ grows to near 1 much earlier in this curve.  This means all of the actives are likely to be identified early on in the curve, when only a small fraction of the data set is tested. This is due to a better separation of the scores sampled from the positive and negative classes.  $\widehat{ACC}(x)$ is a much better estimate of the population accumulation curve when $N = 1000$ and $\pi = 1/101$.

**Figure 6** shows that $\widehat{ACC}(x)$ is an excellent estimator of the population accumulation curve when $N$ is increased to 10000 or the skew is decreased to 1.

```{r, fig.height=10, fig.width=9, fig.cap= 'Estimation of the population ACC curve under the bibeta model with small sample size (N = 1,000) and large skew ($\\pi = 1/101$). $\\alpha_+ = 5$, $\\beta_+ = 1$, $\\alpha_- = 1$, $\\beta_- = 5$.  (top left) The pdf of $S$ (red), $S|+$ (black) and $S|-$ (blue). (top right) Empirical cdf used to estimate P(S > t) (red) and the true population curve (black) as t ranges from 0 to 1.(bottom left) Empirical cdf used to estimate P(S > t | +) (red) and the true population curve (black).  (bottom right) The empirical estimate of the ACC curve (red) and the true population curve (black).'}

#####################
# small pi
#####################

par(mfrow = c(2,2))

t <- seq(0, 1, by = .0001)
pi <- 1/101
p.plus <- dbeta(t, shape1 = 5, shape2 = 1)
p.neg <- dbeta(t, shape1 = 1, shape2 = 5)
plot(t, p.plus, type = "l", xlab = "S", ylab = "Density")
lines(t, p.neg, type = "l", col = "blue")
p.s <- pi*p.plus + (1-pi)*p.neg
lines(t, p.s, type = "l", col = "red")


set.seed(134)
N <- 1000
pi <- 1/101

x <- vector(length = N)
s <- vector(length = N)

# setting up mixture parameters
components <- sample(1:2, prob=c(pi, 1-pi), size=N, replace=TRUE)
# beta parameters
params <- list(c(5, 1), c(1, 5))

# simulate 2
for(i in 1:N){
  s[i] <- rbeta(n = 1, shape1 = params[[components[i]]][1],
                shape2 = params[[components[i]]][2])
}

# simulate x
p.plus <- dbeta(s, shape1 = params[[1]][1], shape2 = params[[1]][2])
p.neg <- dbeta(s, shape1 = params[[2]][1], shape2 = params[[2]][2])
p.s <- pi*p.plus + (1-pi)*p.neg
p.active <- pi*p.plus/p.s  
x <- rbinom(n=N, size = 1, p.active)

t <- seq(1, 0, -.01)
p <- ecdf(s)

emp.p.s <- 1 - p(t)

######## P(S > t)

# Plot of the empirical P(S > t)
plot(t, emp.p.s, type = "l", col = "red", lty = "dashed", 
     xlab = "t", ylab = "P(S > t)", lwd=1.5)

# Plot the actual P(S > t)
p.plus <- pbeta(t, shape1 = params[[1]][1], shape2 = params[[1]][2], lower.tail = F)
p.neg <- pbeta(t, shape1 = params[[2]][1], shape2 = params[[2]][2], lower.tail = F)
true.p.s <- pi*p.plus + (1-pi)*p.neg
lines(t, true.p.s, type = "l")

####### P(S > t | +)

# Plot of the empirical P(S > t | +)
s.plus <- s[components==1]
p <- ecdf(s.plus)
emp.p.s.plus <- 1 - p(t)
plot(t, emp.p.s.plus, type = "l", col = "red", lty = "dashed", 
     xlab = "t", ylab = "P(S > t | +)", lwd=1.5)

# Plot of the true P(S > t | +)
lines(t, p.plus, type = "l", lwd=1.5)

plot(true.p.s, p.plus, type = "l", xlim = c(0,.2),
      xlab = "P(S > t)", ylab = "P(S > t | +)", lwd=1.5)
lines(emp.p.s, emp.p.s.plus, type = "l", col = "red", lty = "dashed", lwd=1.5)
```

```{r, fig.height=8, fig.width=8, fig.cap= 'Estimation of the population ACC curve under the same binormal model as Figure 1 with different combinations of sample size and $\\pi$. (left) Empirical ACC curve estimate when N = 1,000 and $\\pi = 1/2$. (right) Empirical ACC curve estimate when N = 10,000 and $\\pi = 1/101$.'}

#####################
# other curves
#####################

###### N = 1000, pi = .2
par(mfrow = c(1,2))
set.seed(123)
N <- 1000
pi <- .5

x <- vector(length = N)
s <- vector(length = N)

# setting up mixture parameters
components <- sample(1:2, prob=c(pi, 1-pi), size=N, replace=TRUE)
# beta parameters
# params <- list(c(5, 1), c(1, 5))

# simulate 2
for(i in 1:N){
  s[i] <- rbeta(n = 1, shape1 = params[[components[i]]][1],
                shape2 = params[[components[i]]][2])
}

# simulate x
p.plus <- dbeta(s, shape1 = params[[1]][1], shape2 = params[[1]][2])
p.neg <- dbeta(s, shape1 = params[[2]][1], shape2 = params[[2]][2])
p.s <- pi*p.plus + (1-pi)*p.neg
p.active <- pi*p.plus/p.s  
x <- rbinom(n=N, size = 1, p.active)

t <- seq(1, 0, -.01)
p <- ecdf(s)
emp.p.s <- 1 - p(t)

######## P(S > t)

# Plot of the empirical P(S > t)
# Plot the actual P(S > t)
p.plus <- pbeta(t, shape1 = params[[1]][1], shape2 = params[[1]][2], lower.tail = F)
p.neg <- pbeta(t, shape1 = params[[2]][1], shape2 = params[[2]][2], lower.tail = F)
true.p.s <- pi*p.plus + (1-pi)*p.neg

####### P(S > t | +)

# Plot of the empirical P(S > t | +)
s.plus <- s[components==1]
p <- ecdf(s.plus)
emp.p.s.plus <- 1 - p(t)

# Plot of the ACC curve
plot(true.p.s, p.plus, type = "l", xlim = c(0,.7),
      xlab = "P(S > t)", ylab = "P(S > t | +)", lwd=1.5)
lines(emp.p.s, emp.p.s.plus, type = "l", col = "red", lty = "dashed", lwd=1.5)


###################################################


####### N = 10000, pi = 1/101

set.seed(123)
N <- 10000
pi <- 1/101

x <- vector(length = N)
s <- vector(length = N)

# setting up mixture parameters
components <- sample(1:2, prob=c(pi, 1-pi), size=N, replace=TRUE)
# beta parameters
# params <- list(c(5, 1), c(1, 5))

# simulate 2
for(i in 1:N){
  s[i] <- rbeta(n = 1, shape1 = params[[components[i]]][1],
                shape2 = params[[components[i]]][2])
}

# simulate x
p.plus <- dbeta(s, shape1 = params[[1]][1], shape2 = params[[1]][2])
p.neg <- dbeta(s, shape1 = params[[2]][1], shape2 = params[[2]][2])
p.s <- pi*p.plus + (1-pi)*p.neg
p.active <- pi*p.plus/p.s  
x <- rbinom(n=N, size = 1, p.active)

t <- seq(1, 0, -.01)
p <- ecdf(s)
emp.p.s <- 1 - p(t)

######## P(S > t)

# Plot of the empirical P(S > t)
# Plot the actual P(S > t)
p.plus <- pbeta(t, shape1 = params[[1]][1], shape2 = params[[1]][2], lower.tail = F)
p.neg <- pbeta(t, shape1 = params[[2]][1], shape2 = params[[2]][2], lower.tail = F)
true.p.s <- pi*p.plus + (1-pi)*p.neg

####### P(S > t | +)

# Plot of the empirical P(S > t | +)
s.plus <- s[components==1]
p <- ecdf(s.plus)
emp.p.s.plus <- 1 - p(t)

# Plot of the ACC curve
plot(true.p.s, p.plus, type = "l", xlim = c(0,.2),
      xlab = "P(S > t)", ylab = "P(S > t | +)", lwd=1.5)
lines(emp.p.s, emp.p.s.plus, type = "l", col = "red", lty = "dashed", lwd=1.5)

```


## Larger scale simulation

Under both models, for $\pi = 1/101, 1/2$ and $N = 1000, 5000, 10000$, $\widehat{ACC}(x)$ was estimated at x = .01, .1, .5, .9. The top 1 and top 10 percent of the data set was considered, as researchers are often most interested in testing a small top ranked fraction of the data.  Therefore, the quality of $\widehat{ACC}(x)$ early on in the curve was of great interest. 1000 simulation replicates were performed.  Convergence rate to the normal distribution of the $\widehat{ACC}(x)$ estimates was assessed and $MSE$ was computed for the $\widehat{ACC}(x)$ estimator of the ${ACC}(x)$.  To compute the population ${ACC}(x)$, the inverse of the mixture distribution $F_S(\cdot)$ needed to be computed.  In order to do so, I took many samples (N = 1000000) from $F_S(\cdot)$ and then estimated the desired quantile using the sample distribution.

### Binormal

**Figure 7** shows the histograms of $\widehat{ACC}(x)$ estimates under the binormal model with large skew, $\pi = 1/101$.  
Non-parameteric kernel densities were estimated for the empirical $\widehat{ACC}(x)$ using a gaussian kernel with Silverman's rule of thumb as the bandwidth parameter [@Silverman]. Kernel density estimates demonstrated the quality of the normal approximation.  When $\pi = 1/101$, the normal approximation is good at small values of x, even n is small.  However, the approximation is very poor when x = .9.  As n increases the normal approximation improves. **Figure 8** shows that when $\pi = 1/2$, the normal approximation when x = .01 is poor for all n considered.

**Table 1** shows the $MSE$ values for the binormal model. As the skewness decreases to 1, the MSEs decrease substantially, suggesting a decrease in the sampling variance of the $\widehat{ACC}(x)$ curve. When $\pi = 1/2$, x = .5 results in the highest MSE, while the highest MSE was often observed for x = .01 when $\pi = 1/101$.  Increasing N decreases the MSE.

### Bibeta

Many of the same trends were observed under the bibeta model.  Interestingly, the normal approximation of $\widehat{ACC}(x)$ seems to perform much better for the bibeta model, but the MSEs are on average substantially higher for all x.

```{r, cache = T, fig.cap= 'Histograms of observed $\\widehat{ACC}(x)$ for binormal model with large skew ($\\pi = 1/101$). 1000 simulation replicates were performed.  Kernel density estimates are overlayed to demonstrate cases in which there was approximate normality. A gaussian kernel (rows) N = 1000, 5000, 10000. (columns) x = .01, .1, .5, .9.'}

############ Compute the true acc(x)

# Need to get quantiles
set.seed(123)
N <- 1000000
pi <- 1/101
mus <- c(.6, .4)
sd.s <- .1

t <- c(.01, seq(.1, 1, .1))

x <- vector(length = N)
s <- vector(length = N)

# setting up mixture parameters
components <- sample(1:2, prob=c(pi, 1-pi), size=N, replace=TRUE)

# simulate s
s <- rnorm(n = N, mean = mus[components], sd = sd.s)

# got estimate of quantile function
f.s.inv <- quantile(s, probs = 1-t)
acc.truth <- 1 - pnorm(f.s.inv, mean = mus[1], sd = sd.s)

######## Compute the estimated acc(x)

N <- c(1000, 5000, 10000)
reps <- 1000
acc.est.mat <- matrix(ncol = length(t), nrow = reps)
acc.est.ls <- list(acc.est.mat, acc.est.mat, acc.est.mat)
for(j in 1:3) {

  x <- vector(length = N[j])
  s <- vector(length = N[j])
  
  # setting up mixture parameters
  components <- sample(1:2, prob=c(pi, 1-pi), size=N[j], replace=TRUE)
  
  # set up grid
  for(i in 1:reps) {
    # simulate S
    s <- rnorm(n = N[j], mean = mus[components], sd = sd.s)
    
    # simulate x
    p.plus <- dnorm(s, mean = mus[1], sd = sd.s)
    p.neg <- dnorm(s, mean = mus[2], sd = sd.s)
    p.s <- pi*p.plus + (1-pi)*p.neg
    p.active <- pi*p.plus/p.s  
    x <- rbinom(n=N[j], size = 1, p.active)
    
    # Compute the empirical acc(x)
    s.plus <- s[components==1]
    f.plus <- ecdf(s.plus)
    
    f.s.inv <- quantile(s, probs = 1-t)
    acc.est.ls[[j]][i, ] <- 1 - f.plus(f.s.inv)  
  }
}

hist.cols <- c(1,2,6,10)


par(mfrow = c(3, 4), mai = c(.3, 0.3, 0.3, 0.3))
for(j in 1:3){
  for(i in hist.cols){
    Y <- acc.est.ls[[j]][, i]
    hist(Y, prob = T, col="grey", main = "",
         xlab = "Estimate of ACC(x)")
    lines(density(Y, kernel = c("gaussian")), col="blue", lwd=2)
  }
}

```

```{r}
MSE.mat.1 <- matrix(ncol = 4, nrow = 3)

for(j in 1:3){
  for(i in 1:4){
    k <- hist.cols[i]
    MSE.mat.1[j, i] <- mean((acc.truth[k] - acc.est.ls[[j]][, k])^2)
  }
}
```


```{r, cache = T, fig.cap= 'Histograms of observed $\\widehat{ACC}(x)$ for binormal model with skew = 1 ($\\pi = 1/2$). 1000 simulation replicates were performed.  Kernel density estimates are overlayed to demonstrate cases in which there was approximate normality. (rows) N = 1000, 5000, 10000. (columns) x = .01, .1, .5, .9.'}

############ Compute the true acc(x)

# Need to get quantiles

N <- 1000000
pi <- 1/2
mus <- c(.6, .4)
sd.s <- .1

t <- c(.01, seq(.1, 1, .1))

x <- vector(length = N)
s <- vector(length = N)

# setting up mixture parameters
components <- sample(1:2, prob=c(pi, 1-pi), size=N, replace=TRUE)


# simulate s
s <- rnorm(n = N, mean = mus[components], sd = sd.s)

# got estimate of quantile function
f.s.inv <- quantile(s, probs = 1-t)
acc.truth <- 1 - pnorm(f.s.inv, mean = mus[1], sd = sd.s)

######## Compute the estimated acc(x)

N <- c(1000, 5000, 10000)
reps <- 1000
acc.est.mat <- matrix(ncol = length(t), nrow = reps)
acc.est.ls <- list(acc.est.mat, acc.est.mat, acc.est.mat)
for(j in 1:3) {
  x <- vector(length = N[j])
  s <- vector(length = N[j])
  
  # setting up mixture parameters
  components <- sample(1:2, prob=c(pi, 1-pi), size=N[j], replace=TRUE)
  
  # set up grid
  for(i in 1:reps) {
    # simulate S
    s <- rnorm(n = N[j], mean = mus[components], sd = sd.s)
    
    # simulate x
    p.plus <- dnorm(s, mean = mus[1], sd = sd.s)
    p.neg <- dnorm(s, mean = mus[2], sd = sd.s)
    p.s <- pi*p.plus + (1-pi)*p.neg
    p.active <- pi*p.plus/p.s  
    x <- rbinom(n=N[j], size = 1, p.active)
    
    # Compute the empirical acc(x)
    s.plus <- s[components==1]
    f.plus <- ecdf(s.plus)
    
    f.s.inv <- quantile(s, probs = 1-t)
    acc.est.ls[[j]][i, ] <- 1 - f.plus(f.s.inv)  
  }
}

hist.cols <- c(1,2,6,10)


par(mfrow = c(3, 4), mai = c(.3, 0.3, 0.3, 0.3))
for(j in 1:3){
  for(i in hist.cols){
    Y <- acc.est.ls[[j]][, i]
    hist(Y, prob = T, col="grey", main = "",
         xlab = "Estimate of ACC(x)")
    lines(density(Y, kernel = c("gaussian")), col="blue", lwd=2)
  }
}

```

```{r}
MSE.mat.2 <- matrix(ncol = 4, nrow = 3)

for(j in 1:3){
  for(i in 1:4){
    k <- hist.cols[i]
    MSE.mat.2[j, i] <- mean((acc.truth[k] - acc.est.ls[[j]][, k])^2)
  }
}

MSE.mat <- cbind(MSE.mat.1, MSE.mat.2) 

df <- format(MSE.mat, digits = 3)
df <- as.data.frame(df)
colnames(df) <- rep(c(".01", ".1", ".5", ".9"), 2)
rownames(df) <- c("N = 1000", "N = 5000", "N = 10000")

kable(df, format = "latex", align = "c", booktabs = T, caption = "MSE for the $\\widehat{ACC}(x)$ estimator for the binormal model. On the columns are the $x_{acc}$ values considered. Both cases of large skew ($\\pi = 1/101$) and skew = 1 are included.", escape = T) %>% kable_styling(latex_options = c("striped", "scale_down"), position = "center") %>% add_header_above(c(" ", "pi = 1/101" = 4, "pi = 1/2" = 4), italic = T, bold = T) %>% column_spec(1, bold = T) %>% column_spec(5, border_right = T)

```



```{r, cache = T, fig.cap= 'Histograms of observed $\\widehat{ACC}(x)$ for bibeta model with large skew ($\\pi = 1/101$). 1000 simulation replicates were performed.  Kernel density estimates are overlayed to demonstrate cases in which there was approximate normality. (rows) N = 1000, 5000, 10000. (columns) x = .01, .1, .5, .9.'}

############ Compute the true acc(x)

# Need to get quantiles

N <- 1000000
pi <- 1/101
params <- list(c(5, 1), c(1, 5))

t <- c(.01, seq(.1, 1, .1))

x <- vector(length = N)
s <- vector(length = N)

# setting up mixture parameters
components <- sample(1:2, prob=c(pi, 1-pi), size=N, replace=TRUE)

# simulate S
for(i in 1:N){
  s[i] <- rbeta(n = 1, shape1 = params[[components[i]]][1],
                shape2 = params[[components[i]]][2])
}

# got estimate of quantile function
f.s.inv <- quantile(s, probs = 1-t)
acc.truth <- 1 - pbeta(f.s.inv, shape1 = params[[1]][1], shape2 = params[[1]][2])

######## Compute the estimated acc(x)

N <- c(1000, 5000, 10000)
reps <- 1000
acc.est.mat <- matrix(ncol = length(t), nrow = reps)
acc.est.ls <- list(acc.est.mat, acc.est.mat, acc.est.mat)

for(j in 1:3) {
  x <- vector(length = N[j])
  s <- vector(length = N[j])
  
  # setting up mixture parameters
  components <- sample(1:2, prob=c(pi, 1-pi), size=N[j], replace=TRUE)
  
  # set up grid
  for(i in 1:reps) {
    # simulate S
    for(k in 1:N[j]){
      s[k] <- rbeta(n = 1, shape1 = params[[components[i]]][1],
                      shape2 = params[[components[i]]][2])
    }
        
    # simulate x
    p.plus <- dbeta(s, shape1 = params[[1]][1], shape2 = params[[1]][2])
    p.neg <- dbeta(s, shape1 = params[[2]][1], shape2 = params[[2]][2])
    p.s <- pi*p.plus + (1-pi)*p.neg
    p.active <- pi*p.plus/p.s  
    x <- rbinom(n=N[j], size = 1, p.active)
    
    # Compute the empirical acc(x)
    s.plus <- s[components==1]
    f.plus <- ecdf(s.plus)
    
    f.s.inv <- quantile(s, probs = 1-t)
    acc.est.ls[[j]][i, ] <- 1 - f.plus(f.s.inv)  
  }
}

hist.cols <- c(1,2,6,10)

par(mfrow = c(3, 4), mai = c(.3, 0.3, 0.3, 0.3))
for(j in 1:3){
  for(i in hist.cols){
    Y <- acc.est.ls[[j]][, i]
    hist(Y, prob = T, col="grey", main = "",
         xlab = "Estimate of ACC(x)")
    lines(density(Y, kernel = c("gaussian")), col="blue", lwd=2)
  }
}

```

```{r}
MSE.mat.1 <- matrix(ncol = 4, nrow = 3)

for(j in 1:3){
  for(i in 1:4){
    k <- hist.cols[i]
    MSE.mat.1[j, i] <- mean((acc.truth[k] - acc.est.ls[[j]][, k])^2)
  }
}
```

```{r, cache = T, fig.cap= 'Histograms of observed $\\widehat{ACC}(x)$ for bibeta model with skew = 1 ($\\pi = 1/101$). 1000 simulation replicates were performed.  Kernel density estimates are overlayed to demonstrate cases in which there was approximate normality. (rows) N = 1000, 5000, 10000. (columns) x = .01, .1, .5, .9.'}

############ Compute the true acc(x)

# Need to get quantiles

N <- 1000000
pi <- 1/2
params <- list(c(5, 1), c(1, 5))

t <- c(.01, seq(.1, 1, .1))

x <- vector(length = N)
s <- vector(length = N)

# setting up mixture parameters
components <- sample(1:2, prob=c(pi, 1-pi), size=N, replace=TRUE)

# simulate S
for(i in 1:N){
  s[i] <- rbeta(n = 1, shape1 = params[[components[i]]][1],
                shape2 = params[[components[i]]][2])
}

# got estimate of quantile function
f.s.inv <- quantile(s, probs = 1-t)
acc.truth <- 1 - pbeta(f.s.inv, shape1 = params[[1]][1], shape2 = params[[1]][2])

######## Compute the estimated acc(x)

N <- c(1000, 5000, 10000)
reps <- 1000
acc.est.mat <- matrix(ncol = length(t), nrow = reps)
acc.est.ls <- list(acc.est.mat, acc.est.mat, acc.est.mat)

for(j in 1:3) {
  x <- vector(length = N[j])
  s <- vector(length = N[j])
  
  # setting up mixture parameters
  components <- sample(1:2, prob=c(pi, 1-pi), size=N[j], replace=TRUE)
  
  # set up grid
  for(i in 1:reps) {
    # simulate S
    for(k in 1:N[j]){
      s[k] <- rbeta(n = 1, shape1 = params[[components[i]]][1],
                      shape2 = params[[components[i]]][2])
    }
        
    # simulate x
    p.plus <- dbeta(s, shape1 = params[[1]][1], shape2 = params[[1]][2])
    p.neg <- dbeta(s, shape1 = params[[2]][1], shape2 = params[[2]][2])
    p.s <- pi*p.plus + (1-pi)*p.neg
    p.active <- pi*p.plus/p.s  
    x <- rbinom(n=N[j], size = 1, p.active)
    
    # Compute the empirical acc(x)
    s.plus <- s[components==1]
    f.plus <- ecdf(s.plus)
    
    f.s.inv <- quantile(s, probs = 1-t)
    acc.est.ls[[j]][i, ] <- 1 - f.plus(f.s.inv)  
  }
}

hist.cols <- c(1,2,6,10)

par(mfrow = c(3, 4), mai = c(.3, 0.3, 0.3, 0.3))
for(j in 1:3){
  for(i in hist.cols){
    Y <- acc.est.ls[[j]][, i]
    hist(Y, prob = T, col="grey", main = "",
         xlab = "Estimate of ACC(x)")
    lines(density(Y, kernel = c("gaussian")), col="blue", lwd=2)
  }
}

```


```{r}
MSE.mat.2 <- matrix(ncol = 4, nrow = 3)

for(j in 1:3){
  for(i in 1:4){
    k <- hist.cols[i]
    MSE.mat.2[j, i] <- mean((acc.truth[k] - acc.est.ls[[j]][, k])^2)
  }
}

MSE.mat <- cbind(MSE.mat.1, MSE.mat.2) 

df <- format(MSE.mat, digits = 3)
df <- as.data.frame(df)
colnames(df) <- rep(c(".01", ".1", ".5", ".9"), 2)
rownames(df) <- c("N = 1000", "N = 5000", "N = 10000")

kable(df, format = "latex", align = "c", booktabs = T, caption = "MSE for the $\\widehat{ACC}(x)$ estimator for the bibeta model. On the columns are the $x_{acc}$ values considered. Both cases of large skew ($\\pi = 1/101$) and skew = 1 are included.", escape = T) %>% kable_styling(latex_options = c("striped", "scale_down"), position = "center") %>% add_header_above(c(" ", "pi = 1/101" = 4, "pi = 1/2" = 4), italic = T, bold = T) %>% column_spec(1, bold = T) %>% column_spec(5, border_right = T)

```

\pagebreak

# References




  








